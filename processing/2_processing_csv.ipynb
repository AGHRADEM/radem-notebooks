{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2: Processing CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"../data_processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV file into a pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_df_time(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the time column to datetime and floor it to seconds, in place.\n",
    "    \"\"\"\n",
    "    df[\"time\"] = pd.to_datetime(df['time']).dt.floor('S')\n",
    "\n",
    "    return df\n",
    "\n",
    "def fix_df_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find and remove duplicates from the dataframe, in place.\n",
    "    \"\"\"\n",
    "    df.drop_duplicates(inplace=True, keep=\"first\")\n",
    "    return df\n",
    "\n",
    "def fix_sorting_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sort the dataframe by time, in place.\n",
    "    \"\"\"\n",
    "    df.sort_values(\"time\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def fix_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fix the dataframe in place.\n",
    "    \"\"\"\n",
    "    fix_df_time(df)\n",
    "    fix_df_duplicates(df)\n",
    "    fix_sorting_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_df_column_names(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Verify that the dataframe has the required columns\n",
    "    \"\"\"\n",
    "\n",
    "    REQUIRED_COLS = ['time', 'event_type', 'channel', 'value']\n",
    "\n",
    "    cols = df.columns\n",
    "    for col in REQUIRED_COLS:\n",
    "        if col not in cols:\n",
    "            raise ValueError(f\"Column {col} not found in dataframe\")\n",
    "\n",
    "def verify_df_sorted(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Verify that the dataframe is sorted by \"time\"\n",
    "    \"\"\"\n",
    "    # Find rows where the 'time' is decreasing from the previous row\n",
    "    not_sorted_mask = df['time'].diff().dt.total_seconds() < 0\n",
    "\n",
    "    # The first row can't be \"not sorted\" by definition, so we can exclude it from the mask\n",
    "    not_sorted_mask.iloc[0] = False\n",
    "\n",
    "    # Filter the DataFrame to find the not sorted rows\n",
    "    not_sorted_rows = df[not_sorted_mask]\n",
    "\n",
    "    if not df['time'].is_monotonic_increasing:\n",
    "        raise ValueError(f\"Dataframe is not sorted by time:\\n{not_sorted_rows}\")\n",
    "\n",
    "def verify_df_time_diffs(df: pd.DataFrame, \n",
    "                         max_diff_tolerance: np.timedelta64 = np.timedelta64(90, 's'), \n",
    "                         min_diff_tolerance: np.timedelta64 = np.timedelta64(500, 'ms')) -> None:\n",
    "    \"\"\"\n",
    "    Verify that the time differences between events are within tolerance.\n",
    "    If time diff >= max_diff_tolerance, just prints the warning (data holes are permitted).\n",
    "    If time diff <= min_diff_tolerance, raises an exception (possible floating point errors).\n",
    "    \n",
    "    Assumes that the dataframe is non-decreasingly sorted by \"time\".  \n",
    "    \n",
    "    There may me multiple groups of events with the same time.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): input dataframe with \"time\" column\n",
    "        max_diff_tolerance (np.timedelta64, optional): max time difference tolerance in ms (warning only)\n",
    "        min_diff_tolerance (np.timedelta64, optional): min time difference tolerance in ms (exception)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: when time differences < min_diff_tolerance (possible floating point errors)\n",
    "    \"\"\"\n",
    "\n",
    "    # get all unique \"time\" values in df\n",
    "    times = df['time'].unique()\n",
    "\n",
    "    # calc time diffs\n",
    "    time_diffs = np.diff(times)\n",
    "\n",
    "    # check if all time diffs are not larger than the tolerance\n",
    "    checks = max_diff_tolerance > time_diffs\n",
    "    if not all(checks):\n",
    "        # find all indexes of unmet conditions\n",
    "        indexes = np.where(checks == False)[0]\n",
    "\n",
    "        # create a dataframe of times\n",
    "        df_times = pd.DataFrame(times, columns=[\"time\"])\n",
    "\n",
    "        # find all holes\n",
    "        holes = [f\"{df_times.iloc[i]['time']} and {df_times.iloc[i + 1]['time']}\" for i in indexes]\n",
    "        \n",
    "        print(\"Found time holes out of tolerance at times:\", *holes, sep='\\n\\t')\n",
    "\n",
    "\n",
    "    # check if all time diffs are not smaller than the tolerance\n",
    "    # (possible floating point errors)\n",
    "    checks = min_diff_tolerance < time_diffs\n",
    "    if not all(checks):\n",
    "        # find all indexes of unmet conditions\n",
    "        indexes = np.where(checks == False)[0]\n",
    "\n",
    "        # create a dataframe of times\n",
    "        df_times = pd.DataFrame(times, columns=[\"time\"])\n",
    "\n",
    "        # find all too close values\n",
    "        too_close = [f\"{df_times.iloc[i]['time']} and {df_times.iloc[i + 1]['time']}\" for i in indexes]\n",
    "        \n",
    "        raise ValueError(\n",
    "            \"Found time values too close to each other at times \" +\n",
    "            \"(possible floating point errors):\\n\\t\" +\n",
    "            \"\\n\\t\".join(too_close))\n",
    "\n",
    "\n",
    "def verify_df_time_counts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Verify that the dataframe has the correct number of events (49) for each time.\n",
    "    \"\"\"\n",
    "    if not all(df[\"time\"].value_counts() == 49):\n",
    "        raise ValueError(\"Incorrect number of events for some times\")\n",
    "    return df\n",
    "\n",
    "def verify_df_time_p_counts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Verify that the dataframe has the correct number of proton events (9) for each time.\n",
    "    \"\"\"\n",
    "    result = df.groupby('time').apply(lambda group: (group['event_type'] == 'p').sum() == 9)\n",
    "    if not all(result):\n",
    "        raise ValueError(\"Incorrect number of proton events for some times\")\n",
    "    return df\n",
    "\n",
    "def verify_df_time_e_counts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Verify that the dataframe has the correct number of electron events (9) for each time.\n",
    "    \"\"\"\n",
    "    result = df.groupby('time').apply(lambda group: (group['event_type'] == 'e').sum() == 9)\n",
    "    if not all(result):\n",
    "        raise ValueError(\"Incorrect number of electron events for some times\")\n",
    "    return df\n",
    "\n",
    "def verify_df_time_d_counts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Verify that the dataframe has the correct number of directional events (31) for each time.\n",
    "    \"\"\"\n",
    "    result = df.groupby('time').apply(lambda group: (group['event_type'] == 'd').sum() == 31)\n",
    "    if not all(result):\n",
    "        raise ValueError(\"Incorrect number of directional events for some times\")\n",
    "    return df\n",
    "\n",
    "def verify_df(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Verify the integrity of the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Dataframe is empty\")\n",
    "\n",
    "    print(\"Verifying column names\")\n",
    "    verify_df_column_names(df)\n",
    "\n",
    "    print(\"Verifying sorting\")\n",
    "    verify_df_sorted(df)\n",
    "\n",
    "    print(\"Verifying time diffs\")\n",
    "    verify_df_time_diffs(df)\n",
    "\n",
    "    print(\"Verifying time counts\")\n",
    "    verify_df_time_counts(df)\n",
    "    print(\"Verifying time p counts\")\n",
    "    verify_df_time_d_counts(df)\n",
    "    print(\"Verifying time e counts\")\n",
    "    verify_df_time_e_counts(df)\n",
    "    print(\"Verifying time d counts\")\n",
    "    verify_df_time_p_counts(df)\n",
    "\n",
    "    # df.groupby('time').apply(verify_df_time_group)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and verifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_verify_csv(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV file into a pandas DataFrame and verify its integrity\n",
    "    \"\"\"\n",
    "\n",
    "    df = load_csv(filename)\n",
    "\n",
    "    verify_df(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_and_verify_csvs(filenames: list) -> list:\n",
    "    \"\"\"\n",
    "    Load a list of CSV files into a list of pandas DataFrames and verify their integrity\n",
    "    \"\"\"\n",
    "\n",
    "    dfs = []\n",
    "    for filename in filenames:\n",
    "        print(\"Verifying\", filename, \"...\")\n",
    "        df = load_and_verify_csv(filename)\n",
    "        dfs.append(df)\n",
    "\n",
    "    print(\"SUCCESS\")\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [load_csv(DIR + str(i) + \".csv\") for i in range(0, 363)]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>event_type</th>\n",
       "      <th>channel</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-16 14:05:35</td>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2023-04-16 14:05:35</td>\n",
       "      <td>d</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2023-04-16 14:05:35</td>\n",
       "      <td>d</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2023-04-16 14:05:35</td>\n",
       "      <td>d</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2023-04-16 14:05:35</td>\n",
       "      <td>d</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42600</th>\n",
       "      <td>2024-04-02 14:29:30</td>\n",
       "      <td>d</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42601</th>\n",
       "      <td>2024-04-02 14:29:30</td>\n",
       "      <td>d</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42602</th>\n",
       "      <td>2024-04-02 14:29:30</td>\n",
       "      <td>d</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42591</th>\n",
       "      <td>2024-04-02 14:29:30</td>\n",
       "      <td>d</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42629</th>\n",
       "      <td>2024-04-02 14:29:30</td>\n",
       "      <td>p</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15061767 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time event_type  channel  value\n",
       "0     2023-04-16 14:05:35          d        1      0\n",
       "27    2023-04-16 14:05:35          d       28      0\n",
       "28    2023-04-16 14:05:35          d       29      0\n",
       "29    2023-04-16 14:05:35          d       30      1\n",
       "30    2023-04-16 14:05:35          d       31     12\n",
       "...                   ...        ...      ...    ...\n",
       "42600 2024-04-02 14:29:30          d       20      0\n",
       "42601 2024-04-02 14:29:30          d       21      1\n",
       "42602 2024-04-02 14:29:30          d       22      3\n",
       "42591 2024-04-02 14:29:30          d       11      9\n",
       "42629 2024-04-02 14:29:30          p        9      0\n",
       "\n",
       "[15061767 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying column names\n",
      "Verifying sorting\n",
      "Verifying time diffs\n",
      "Found time holes out of tolerance at times:\n",
      "\t2023-04-16 14:07:25 and 2023-04-16 14:32:35\n",
      "\t2023-04-16 15:02:52 and 2023-04-16 15:58:35\n",
      "\t2023-04-16 16:00:25 and 2023-04-16 16:07:35\n",
      "\t2023-04-16 16:37:52 and 2023-04-16 16:54:35\n",
      "\t2023-04-16 16:56:25 and 2023-04-16 17:10:35\n",
      "\t2023-04-16 17:40:52 and 2023-04-18 16:08:48\n",
      "\t2023-04-18 16:08:48 and 2023-04-18 16:17:08\n",
      "\t2023-04-18 16:17:08 and 2023-04-18 16:25:28\n",
      "\t2023-04-18 16:25:28 and 2023-04-18 16:33:48\n",
      "\t2023-04-18 16:33:48 and 2023-04-18 16:42:08\n",
      "\t2023-04-18 16:42:08 and 2023-04-18 16:50:28\n",
      "\t2023-04-18 16:50:28 and 2023-04-18 16:58:48\n",
      "\t2023-04-18 16:58:48 and 2023-04-18 17:07:08\n",
      "\t2023-04-18 17:07:08 and 2023-04-18 17:15:28\n",
      "\t2023-04-18 17:15:28 and 2023-04-18 17:23:48\n",
      "\t2023-04-18 17:23:48 and 2023-04-18 17:32:08\n",
      "\t2023-04-18 17:32:08 and 2023-04-18 17:40:52\n",
      "\t2023-04-18 17:40:52 and 2023-04-18 17:49:12\n",
      "\t2023-04-18 17:49:12 and 2023-04-18 17:57:32\n",
      "\t2023-04-18 17:57:32 and 2023-04-18 18:05:52\n",
      "\t2023-04-18 18:05:52 and 2023-04-18 18:14:12\n",
      "\t2023-04-18 18:14:12 and 2023-04-18 18:22:32\n",
      "\t2023-04-18 18:22:32 and 2023-04-18 18:30:52\n",
      "\t2023-04-18 18:30:52 and 2023-04-18 18:39:12\n",
      "\t2023-04-18 18:39:12 and 2023-04-18 18:47:32\n",
      "\t2023-04-18 18:47:32 and 2023-04-18 18:55:52\n",
      "\t2023-04-18 18:55:52 and 2023-04-18 19:04:12\n",
      "\t2023-04-18 19:04:12 and 2023-04-18 19:12:54\n",
      "\t2023-04-18 19:12:54 and 2023-04-18 19:21:14\n",
      "\t2023-04-18 19:21:14 and 2023-04-18 19:29:34\n",
      "\t2023-04-18 19:29:34 and 2023-04-18 19:37:54\n",
      "\t2023-04-18 19:37:54 and 2023-04-18 19:46:14\n",
      "\t2023-04-18 19:46:14 and 2023-04-18 19:54:34\n",
      "\t2023-04-18 19:54:34 and 2023-04-18 20:02:54\n",
      "\t2023-04-18 20:02:54 and 2023-04-18 20:11:14\n",
      "\t2023-04-18 20:11:14 and 2023-04-18 20:19:34\n",
      "\t2023-04-18 20:19:34 and 2023-04-18 20:27:54\n",
      "\t2023-04-18 20:27:54 and 2023-04-18 20:36:14\n",
      "\t2023-04-18 20:36:14 and 2023-04-18 20:44:57\n",
      "\t2023-04-18 20:44:57 and 2023-04-18 20:53:17\n",
      "\t2023-04-18 20:53:17 and 2023-04-18 21:01:37\n",
      "\t2023-04-18 21:01:37 and 2023-04-18 21:09:57\n",
      "\t2023-04-18 21:09:57 and 2023-04-18 21:18:17\n",
      "\t2023-04-18 21:18:17 and 2023-04-18 21:26:37\n",
      "\t2023-04-18 21:26:37 and 2023-04-18 21:34:57\n",
      "\t2023-04-18 21:34:57 and 2023-04-18 21:43:17\n",
      "\t2023-04-18 21:43:17 and 2023-04-18 21:51:37\n",
      "\t2023-04-18 21:51:37 and 2023-04-18 21:59:57\n",
      "\t2023-04-18 21:59:57 and 2023-04-18 22:08:17\n",
      "\t2023-04-18 22:08:17 and 2023-04-18 22:17:01\n",
      "\t2023-04-18 22:17:01 and 2023-04-18 22:25:21\n",
      "\t2023-04-18 22:25:21 and 2023-04-18 22:33:41\n",
      "\t2023-04-18 22:33:41 and 2023-04-18 22:42:01\n",
      "\t2023-04-18 22:42:01 and 2023-04-18 22:50:21\n",
      "\t2023-04-18 22:50:21 and 2023-04-18 22:58:41\n",
      "\t2023-04-18 22:58:41 and 2023-04-18 23:07:01\n",
      "\t2023-04-18 23:07:01 and 2023-04-18 23:15:21\n",
      "\t2023-04-18 23:15:21 and 2023-04-18 23:23:41\n",
      "\t2023-04-18 23:23:41 and 2023-04-18 23:32:01\n",
      "\t2023-04-18 23:32:01 and 2023-04-18 23:40:21\n",
      "\t2023-04-18 23:40:21 and 2023-04-18 23:49:03\n",
      "\t2023-04-18 23:49:03 and 2023-04-18 23:57:23\n",
      "\t2023-04-18 23:57:23 and 2023-04-19 00:05:43\n",
      "\t2023-04-19 00:05:43 and 2023-04-19 00:14:03\n",
      "\t2023-04-19 00:14:03 and 2023-04-19 00:22:23\n",
      "\t2023-04-19 00:22:23 and 2023-04-19 00:30:43\n",
      "\t2023-04-19 00:30:43 and 2023-04-19 00:39:03\n",
      "\t2023-04-19 00:39:03 and 2023-04-19 00:47:23\n",
      "\t2023-04-19 00:47:23 and 2023-04-19 00:55:43\n",
      "\t2023-04-19 00:55:43 and 2023-04-19 01:04:03\n",
      "\t2023-04-19 01:04:03 and 2023-04-19 01:12:23\n",
      "\t2023-04-19 01:12:23 and 2023-04-19 01:21:06\n",
      "\t2023-04-19 01:21:06 and 2023-04-19 01:29:26\n",
      "\t2023-04-19 01:29:26 and 2023-04-19 01:37:46\n",
      "\t2023-04-19 01:37:46 and 2023-04-19 01:46:06\n",
      "\t2023-04-19 01:46:06 and 2023-04-19 01:54:26\n",
      "\t2023-04-19 01:54:26 and 2023-04-19 02:02:46\n",
      "\t2023-04-19 02:02:46 and 2023-04-19 02:11:06\n",
      "\t2023-04-19 02:11:06 and 2023-04-19 02:19:26\n",
      "\t2023-04-19 02:19:26 and 2023-04-19 02:27:46\n",
      "\t2023-04-19 02:27:46 and 2023-04-19 02:36:06\n",
      "\t2023-04-19 02:36:06 and 2023-04-19 02:44:26\n",
      "\t2023-04-19 02:44:26 and 2023-04-19 02:53:09\n",
      "\t2023-04-19 02:53:09 and 2023-04-19 03:01:29\n",
      "\t2023-04-19 03:01:29 and 2023-04-19 03:09:49\n",
      "\t2023-04-19 03:09:49 and 2023-04-19 03:18:09\n",
      "\t2023-04-19 03:18:09 and 2023-04-19 03:26:29\n",
      "\t2023-04-19 03:26:29 and 2023-04-19 03:34:49\n",
      "\t2023-04-19 03:34:49 and 2023-04-19 03:43:09\n",
      "\t2023-04-19 03:43:09 and 2023-04-19 03:51:29\n",
      "\t2023-04-19 03:51:29 and 2023-04-19 03:59:49\n",
      "\t2023-04-19 03:59:49 and 2023-04-19 04:08:09\n",
      "\t2023-04-19 04:08:09 and 2023-04-19 04:16:29\n",
      "\t2023-04-19 04:16:29 and 2023-04-19 04:25:13\n",
      "\t2023-04-19 04:25:13 and 2023-04-19 04:33:33\n",
      "\t2023-04-19 04:33:33 and 2023-04-19 04:41:53\n",
      "\t2023-04-19 04:41:53 and 2023-04-19 04:50:13\n",
      "\t2023-04-19 04:50:13 and 2023-04-19 04:58:33\n",
      "\t2023-04-19 04:58:33 and 2023-04-19 05:06:53\n",
      "\t2023-04-19 05:06:53 and 2023-04-19 05:15:13\n",
      "\t2023-04-19 05:15:13 and 2023-04-19 05:23:33\n",
      "\t2023-04-19 05:23:33 and 2023-04-19 05:31:53\n",
      "\t2023-04-19 05:31:53 and 2023-04-19 05:40:13\n",
      "\t2023-04-19 05:40:13 and 2023-04-19 05:48:33\n",
      "\t2023-04-19 05:48:33 and 2023-04-19 05:57:15\n",
      "\t2023-04-19 05:57:15 and 2023-04-19 06:05:35\n",
      "\t2023-04-19 06:05:35 and 2023-04-19 06:13:55\n",
      "\t2023-04-19 06:13:55 and 2023-04-19 06:22:15\n",
      "\t2023-04-19 06:22:15 and 2023-04-19 06:30:35\n",
      "\t2023-04-19 06:30:35 and 2023-04-19 06:38:55\n",
      "\t2023-04-19 06:38:55 and 2023-04-19 06:47:15\n",
      "\t2023-04-19 06:47:15 and 2023-04-19 06:55:35\n",
      "\t2023-04-19 06:55:35 and 2023-04-19 07:03:55\n",
      "\t2023-04-19 07:03:55 and 2023-04-19 07:12:15\n",
      "\t2023-04-19 07:12:15 and 2023-04-19 07:20:35\n",
      "\t2023-04-19 07:20:35 and 2023-04-19 07:29:18\n",
      "\t2023-04-19 07:29:18 and 2023-04-19 07:37:38\n",
      "\t2023-04-19 07:37:38 and 2023-04-19 07:45:58\n",
      "\t2023-04-19 07:45:58 and 2023-04-19 07:54:18\n",
      "\t2023-04-19 07:54:18 and 2023-04-19 08:02:38\n",
      "\t2023-04-19 08:02:38 and 2023-04-19 08:10:58\n",
      "\t2023-04-19 08:10:58 and 2023-04-19 08:19:18\n",
      "\t2023-04-19 08:19:18 and 2023-04-19 08:27:38\n",
      "\t2023-04-19 08:27:38 and 2023-04-19 08:35:58\n",
      "\t2023-04-19 08:35:58 and 2023-04-19 08:44:18\n",
      "\t2023-04-19 08:44:18 and 2023-04-19 08:52:38\n",
      "\t2023-04-19 08:52:38 and 2023-04-19 09:01:42\n",
      "\t2023-04-19 09:01:42 and 2023-04-19 09:10:02\n",
      "\t2023-04-19 09:10:02 and 2023-04-19 09:18:22\n",
      "\t2023-04-19 09:18:22 and 2023-04-19 09:26:42\n",
      "\t2023-04-19 09:26:42 and 2023-04-19 09:35:02\n",
      "\t2023-04-19 09:35:02 and 2023-04-19 09:43:22\n",
      "\t2023-04-19 09:43:22 and 2023-04-19 09:51:42\n",
      "\t2023-04-19 09:51:42 and 2023-04-19 10:00:02\n",
      "\t2023-04-19 10:00:02 and 2023-04-19 10:08:22\n",
      "\t2023-04-19 10:08:22 and 2023-04-19 10:16:42\n",
      "\t2023-04-19 10:16:42 and 2023-04-19 10:25:02\n",
      "\t2023-04-19 10:25:02 and 2023-04-19 10:33:44\n",
      "\t2023-04-19 10:33:44 and 2023-04-19 10:42:04\n",
      "\t2023-04-19 10:42:04 and 2023-04-19 10:50:24\n",
      "\t2023-04-19 10:50:24 and 2023-04-19 10:58:44\n",
      "\t2023-04-19 10:58:44 and 2023-04-19 11:07:04\n",
      "\t2023-04-19 11:07:04 and 2023-04-19 11:15:24\n",
      "\t2023-04-19 11:15:24 and 2023-04-19 11:23:44\n",
      "\t2023-04-19 11:23:44 and 2023-04-19 11:32:04\n",
      "\t2023-04-19 11:32:04 and 2023-04-19 11:40:24\n",
      "\t2023-04-19 11:40:24 and 2023-04-19 11:48:44\n",
      "\t2023-04-19 11:48:44 and 2023-04-19 11:57:04\n",
      "\t2023-04-19 11:57:04 and 2023-04-19 12:05:47\n",
      "\t2023-04-19 12:05:47 and 2023-04-19 12:14:07\n",
      "\t2023-04-19 12:14:07 and 2023-04-19 12:22:27\n",
      "\t2023-04-19 12:22:27 and 2023-04-19 12:30:47\n",
      "\t2023-04-19 12:30:47 and 2023-04-19 12:39:07\n",
      "\t2023-04-19 12:39:07 and 2023-04-19 12:47:27\n",
      "\t2023-04-19 12:47:27 and 2023-04-19 12:55:47\n",
      "\t2023-04-19 12:55:47 and 2023-04-19 13:04:07\n",
      "\t2023-04-19 13:04:07 and 2023-04-19 13:12:27\n",
      "\t2023-04-19 13:12:27 and 2023-04-19 13:20:47\n",
      "\t2023-04-19 13:20:47 and 2023-04-19 13:29:07\n",
      "\t2023-04-19 13:29:07 and 2023-04-19 13:37:27\n",
      "\t2023-04-19 13:37:27 and 2023-04-19 13:45:47\n",
      "\t2023-04-19 13:45:47 and 2023-04-19 13:54:07\n",
      "\t2023-04-19 13:54:07 and 2023-04-19 14:02:27\n",
      "\t2023-04-19 14:02:27 and 2023-04-19 14:10:47\n",
      "\t2023-04-19 14:10:47 and 2023-04-19 14:19:07\n",
      "\t2023-04-19 14:19:07 and 2023-04-19 14:27:27\n",
      "\t2023-04-19 14:27:27 and 2023-04-19 14:35:47\n",
      "\t2023-04-19 14:35:47 and 2023-04-19 14:44:07\n",
      "\t2023-04-19 14:44:07 and 2023-04-19 14:52:27\n",
      "\t2023-04-19 14:52:27 and 2023-04-19 15:00:47\n",
      "\t2023-04-19 15:00:47 and 2023-04-19 15:09:54\n",
      "\t2023-04-19 15:09:54 and 2023-04-19 15:18:14\n",
      "\t2023-04-19 15:18:14 and 2023-04-19 15:26:34\n",
      "\t2023-04-19 15:26:34 and 2023-04-19 15:34:54\n",
      "\t2023-04-19 15:34:54 and 2023-04-19 15:43:14\n",
      "\t2023-04-19 15:43:14 and 2023-04-19 15:51:34\n",
      "\t2023-04-19 15:51:34 and 2023-04-19 15:59:55\n",
      "\t2023-04-19 15:59:55 and 2023-04-19 16:08:14\n",
      "\t2023-04-19 16:08:14 and 2023-04-19 16:16:34\n",
      "\t2023-04-19 16:16:34 and 2023-04-19 16:24:54\n",
      "\t2023-04-19 16:24:54 and 2023-04-19 16:33:14\n",
      "\t2023-04-19 16:33:14 and 2023-04-19 16:41:56\n",
      "\t2023-04-19 16:41:56 and 2023-04-19 16:50:16\n",
      "\t2023-04-19 16:50:16 and 2023-04-19 16:58:36\n",
      "\t2023-04-19 16:58:36 and 2023-04-19 17:06:56\n",
      "\t2023-04-19 17:06:56 and 2023-04-19 17:15:16\n",
      "\t2023-04-19 17:15:16 and 2023-04-19 17:23:36\n",
      "\t2023-04-19 17:23:36 and 2023-04-19 17:31:56\n",
      "\t2023-04-19 17:31:56 and 2023-04-19 17:40:16\n",
      "\t2023-04-19 17:40:16 and 2023-04-19 17:48:36\n",
      "\t2023-04-19 17:48:36 and 2023-04-19 17:56:56\n",
      "\t2023-04-19 17:56:56 and 2023-04-19 18:05:16\n",
      "\t2023-04-19 18:05:16 and 2023-04-19 18:13:59\n",
      "\t2023-04-19 18:13:59 and 2023-04-19 18:22:19\n",
      "\t2023-04-19 18:22:19 and 2023-04-19 18:30:39\n",
      "\t2023-04-19 18:30:39 and 2023-04-19 18:38:59\n",
      "\t2023-04-19 18:38:59 and 2023-04-19 18:47:19\n",
      "\t2023-04-19 18:47:19 and 2023-04-19 18:55:39\n",
      "\t2023-04-19 18:55:39 and 2023-04-19 19:03:59\n",
      "\t2023-04-19 19:03:59 and 2023-04-19 19:12:19\n",
      "\t2023-04-19 19:12:19 and 2023-04-19 19:20:39\n",
      "\t2023-04-19 19:20:39 and 2023-04-19 19:28:59\n",
      "\t2023-04-19 19:28:59 and 2023-04-19 19:37:19\n",
      "\t2023-04-19 19:37:19 and 2023-04-19 19:46:03\n",
      "\t2023-04-19 19:46:03 and 2023-04-19 19:54:23\n",
      "\t2023-04-19 19:54:23 and 2023-04-19 20:02:43\n",
      "\t2023-04-19 20:02:43 and 2023-04-19 20:11:03\n",
      "\t2023-04-19 20:11:03 and 2023-04-19 20:19:23\n",
      "\t2023-04-19 20:19:23 and 2023-04-19 20:27:43\n",
      "\t2023-04-19 20:27:43 and 2023-04-19 20:36:03\n",
      "\t2023-04-19 20:36:03 and 2023-04-19 20:44:23\n",
      "\t2023-04-19 20:44:23 and 2023-04-19 20:52:43\n",
      "\t2023-04-19 20:52:43 and 2023-04-19 21:01:03\n",
      "\t2023-04-19 21:01:03 and 2023-04-19 21:09:23\n",
      "\t2023-04-19 21:09:23 and 2023-04-19 21:18:05\n",
      "\t2023-04-19 21:18:05 and 2023-04-19 21:26:25\n",
      "\t2023-04-19 21:26:25 and 2023-04-19 21:34:45\n",
      "\t2023-04-19 21:34:45 and 2023-04-19 21:43:05\n",
      "\t2023-04-19 21:43:05 and 2023-04-19 21:51:25\n",
      "\t2023-04-19 21:51:25 and 2023-04-19 21:59:45\n",
      "\t2023-04-19 21:59:45 and 2023-04-19 22:08:05\n",
      "\t2023-04-19 22:08:05 and 2023-04-19 22:16:25\n",
      "\t2023-04-19 22:16:25 and 2023-04-19 22:24:45\n",
      "\t2023-04-19 22:24:45 and 2023-04-19 22:33:05\n",
      "\t2023-04-19 22:33:05 and 2023-04-19 22:41:25\n",
      "\t2023-04-19 22:41:25 and 2023-04-19 22:50:08\n",
      "\t2023-04-19 22:50:08 and 2023-04-19 22:58:28\n",
      "\t2023-04-19 22:58:28 and 2023-04-19 23:06:48\n",
      "\t2023-04-19 23:06:48 and 2023-04-19 23:15:08\n",
      "\t2023-04-19 23:15:08 and 2023-04-19 23:23:28\n",
      "\t2023-04-19 23:23:28 and 2023-04-19 23:31:48\n",
      "\t2023-04-19 23:31:48 and 2023-04-19 23:40:08\n",
      "\t2023-04-19 23:40:08 and 2023-04-19 23:48:28\n",
      "\t2023-04-19 23:48:28 and 2023-04-19 23:56:48\n",
      "\t2023-04-19 23:56:48 and 2023-04-20 00:05:08\n",
      "\t2023-04-20 00:05:08 and 2023-04-20 00:13:28\n",
      "\t2023-04-20 00:13:28 and 2023-04-20 00:22:11\n",
      "\t2023-04-20 00:22:11 and 2023-04-20 00:30:31\n",
      "\t2023-04-20 00:30:31 and 2023-04-20 00:38:51\n",
      "\t2023-04-20 00:38:51 and 2023-04-20 00:47:11\n",
      "\t2023-04-20 00:47:11 and 2023-04-20 00:55:31\n",
      "\t2023-04-20 00:55:31 and 2023-04-20 01:03:51\n",
      "\t2023-04-20 01:03:51 and 2023-04-20 01:12:11\n",
      "\t2023-04-20 01:12:11 and 2023-04-20 01:20:31\n",
      "\t2023-04-20 01:20:31 and 2023-04-20 01:28:51\n",
      "\t2023-04-20 01:28:51 and 2023-04-20 01:37:11\n",
      "\t2023-04-20 01:37:11 and 2023-04-20 01:45:31\n",
      "\t2023-04-20 01:45:31 and 2023-04-24 10:17:08\n",
      "\t2023-04-24 10:17:08 and 2023-04-24 10:33:48\n",
      "\t2023-04-24 10:33:48 and 2023-04-24 10:50:28\n",
      "\t2023-04-24 10:50:28 and 2023-04-24 11:07:08\n",
      "\t2023-04-24 11:07:08 and 2023-04-24 11:23:48\n",
      "\t2023-04-24 11:23:48 and 2023-04-24 11:40:28\n",
      "\t2023-04-24 11:40:28 and 2023-04-24 11:57:08\n",
      "\t2023-04-24 11:57:08 and 2023-04-24 12:13:48\n",
      "\t2023-04-24 12:13:48 and 2023-04-24 12:30:28\n",
      "\t2023-04-24 12:30:28 and 2023-04-24 12:47:08\n",
      "\t2023-04-24 12:47:08 and 2023-04-24 13:03:48\n",
      "\t2023-04-24 13:03:48 and 2023-04-24 13:20:28\n",
      "\t2023-04-24 13:20:28 and 2023-04-24 13:37:08\n",
      "\t2023-04-24 13:37:08 and 2023-04-24 13:53:48\n",
      "\t2023-04-24 13:53:48 and 2023-04-24 14:10:28\n",
      "\t2023-04-24 14:10:28 and 2023-04-24 14:27:08\n",
      "\t2023-04-24 14:27:08 and 2023-04-24 14:43:48\n",
      "\t2023-04-24 14:43:48 and 2023-04-24 15:00:28\n",
      "\t2023-04-24 15:00:28 and 2023-04-24 15:17:08\n",
      "\t2023-04-24 15:17:08 and 2023-04-24 15:33:48\n",
      "\t2023-04-24 15:33:48 and 2023-04-24 15:50:28\n",
      "\t2023-04-24 15:50:28 and 2023-04-24 16:07:08\n",
      "\t2023-04-24 16:07:08 and 2023-04-24 16:23:48\n",
      "\t2023-04-24 16:23:48 and 2023-04-24 16:40:28\n",
      "\t2023-04-24 16:40:28 and 2023-04-24 16:57:08\n",
      "\t2023-04-24 16:57:08 and 2023-04-24 17:13:48\n",
      "\t2023-04-24 17:13:48 and 2023-04-24 17:30:28\n",
      "\t2023-04-24 17:30:28 and 2023-04-24 17:47:08\n",
      "\t2023-04-24 17:47:08 and 2023-04-24 18:03:48\n",
      "\t2023-04-24 18:03:48 and 2023-04-24 18:20:28\n",
      "\t2023-04-24 18:20:28 and 2023-04-24 18:37:08\n",
      "\t2023-04-24 18:37:08 and 2023-04-24 18:53:48\n",
      "\t2023-04-24 18:53:48 and 2023-04-24 19:10:29\n",
      "\t2023-04-24 19:10:29 and 2023-04-24 19:27:09\n",
      "\t2023-04-24 19:27:09 and 2023-04-24 19:43:49\n",
      "\t2023-04-24 19:43:49 and 2023-04-24 20:00:29\n",
      "\t2023-04-24 20:00:29 and 2023-04-24 20:17:09\n",
      "\t2023-04-24 20:17:09 and 2023-04-24 20:33:49\n",
      "\t2023-04-24 20:33:49 and 2023-04-24 20:50:29\n",
      "\t2023-04-24 20:50:29 and 2023-04-24 21:07:09\n",
      "\t2023-04-24 21:07:09 and 2023-04-24 21:23:49\n",
      "\t2023-04-24 21:23:49 and 2023-04-24 21:40:29\n",
      "\t2023-04-24 21:40:29 and 2023-04-24 21:57:09\n",
      "\t2023-04-24 21:57:09 and 2023-04-24 22:13:49\n",
      "\t2023-04-24 22:13:49 and 2023-04-24 22:30:29\n",
      "\t2023-04-24 22:30:29 and 2023-04-24 22:47:09\n",
      "\t2023-04-24 22:47:09 and 2023-04-24 23:03:49\n",
      "\t2023-04-24 23:03:49 and 2023-04-24 23:20:29\n",
      "\t2023-04-24 23:20:29 and 2023-04-24 23:37:09\n",
      "\t2023-04-24 23:37:09 and 2023-04-24 23:53:49\n",
      "\t2023-04-24 23:53:49 and 2023-04-25 00:10:29\n",
      "\t2023-04-25 00:10:29 and 2023-04-25 00:27:09\n",
      "\t2023-04-25 00:27:09 and 2023-04-25 00:43:49\n",
      "\t2023-04-25 00:43:49 and 2023-04-25 01:00:29\n",
      "\t2023-04-25 01:00:29 and 2023-04-25 01:17:09\n",
      "\t2023-04-25 01:17:09 and 2023-04-25 01:33:49\n",
      "\t2023-04-25 01:33:49 and 2023-04-25 01:50:29\n",
      "\t2023-04-25 01:50:29 and 2023-04-25 02:07:09\n",
      "\t2023-04-25 02:07:09 and 2023-04-25 02:23:49\n",
      "\t2023-04-25 02:23:49 and 2023-04-25 02:40:29\n",
      "\t2023-04-25 02:40:29 and 2023-04-25 02:57:09\n",
      "\t2023-04-25 02:57:09 and 2023-04-25 03:13:49\n",
      "\t2023-04-25 03:13:49 and 2023-04-25 03:30:29\n",
      "\t2023-04-25 03:30:29 and 2023-04-25 03:47:09\n",
      "\t2023-04-25 03:47:09 and 2023-04-25 04:03:49\n",
      "\t2023-04-25 04:03:49 and 2023-04-25 04:20:29\n",
      "\t2023-04-25 04:20:29 and 2023-04-25 04:37:09\n",
      "\t2023-04-25 04:37:09 and 2023-04-25 04:53:49\n",
      "\t2023-04-25 04:53:49 and 2023-04-25 05:10:29\n",
      "\t2023-04-25 05:10:29 and 2023-04-25 05:27:09\n",
      "\t2023-04-25 05:27:09 and 2023-04-25 05:43:49\n",
      "\t2023-04-25 05:43:49 and 2023-04-25 06:00:29\n",
      "\t2023-04-25 06:00:29 and 2023-04-25 06:17:09\n",
      "\t2023-04-25 06:17:09 and 2023-04-25 06:33:49\n",
      "\t2023-04-25 06:33:49 and 2023-04-25 06:50:29\n",
      "\t2023-04-25 06:50:29 and 2023-04-25 07:07:09\n",
      "\t2023-04-25 07:07:09 and 2023-04-25 07:23:49\n",
      "\t2023-04-25 07:23:49 and 2023-04-25 07:40:29\n",
      "\t2023-04-25 07:40:29 and 2023-04-25 07:57:09\n",
      "\t2023-04-25 07:57:09 and 2023-04-25 08:13:49\n",
      "\t2023-04-25 08:13:49 and 2023-04-25 08:30:29\n",
      "\t2023-04-25 08:30:29 and 2023-04-25 08:47:09\n",
      "\t2023-04-25 08:47:09 and 2023-04-25 09:03:49\n",
      "\t2023-04-25 09:03:49 and 2023-04-25 09:20:29\n",
      "\t2023-04-25 09:20:29 and 2023-04-25 09:37:09\n",
      "\t2023-04-25 09:37:09 and 2023-04-25 09:53:50\n",
      "\t2023-04-25 09:53:50 and 2023-04-25 10:10:30\n",
      "\t2023-04-25 10:10:30 and 2023-04-25 10:27:10\n",
      "\t2023-04-25 10:27:10 and 2023-04-25 10:43:50\n",
      "\t2023-04-25 10:43:50 and 2023-04-25 11:00:30\n",
      "\t2023-04-25 11:00:30 and 2023-04-25 11:17:10\n",
      "\t2023-04-25 11:17:10 and 2023-04-25 11:33:50\n",
      "\t2023-04-25 11:33:50 and 2023-04-25 11:50:30\n",
      "\t2023-04-25 11:50:30 and 2023-04-25 12:07:10\n",
      "\t2023-04-25 12:07:10 and 2023-04-25 12:23:50\n",
      "\t2023-04-25 12:23:50 and 2023-04-25 12:40:30\n",
      "\t2023-04-25 12:40:30 and 2023-04-25 12:57:10\n",
      "\t2023-04-25 12:57:10 and 2023-04-25 13:13:50\n",
      "\t2023-04-25 13:13:50 and 2023-04-25 13:30:30\n",
      "\t2023-04-25 13:30:30 and 2023-04-25 13:47:10\n",
      "\t2023-04-25 13:47:10 and 2023-04-25 14:03:50\n",
      "\t2023-04-25 14:03:50 and 2023-07-07 05:17:03\n",
      "\t2023-07-07 05:25:03 and 2023-07-09 04:07:03\n",
      "\t2023-07-09 04:15:03 and 2023-07-13 04:27:03\n",
      "\t2023-07-13 04:35:03 and 2023-08-31 18:53:55\n",
      "\t2024-01-26 21:02:24 and 2024-01-26 21:04:24\n",
      "\t2024-02-01 17:05:35 and 2024-02-01 17:07:35\n",
      "\t2024-02-01 17:10:35 and 2024-02-01 17:15:35\n",
      "\t2024-03-04 13:27:39 and 2024-03-06 13:15:39\n",
      "\t2024-03-19 08:03:03 and 2024-03-19 08:06:03\n",
      "Verifying time counts\n",
      "Verifying time p counts\n",
      "Verifying time e counts\n",
      "Verifying time d counts\n"
     ]
    }
   ],
   "source": [
    "verify_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

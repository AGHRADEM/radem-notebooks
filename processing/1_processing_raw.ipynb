{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1: Processing raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "if os.environ.get('CDF_LIB', '') == '':\n",
    "    print('No CDF_LIB environment variable found for CDF file processing.')\n",
    "from spacepy import pycdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW = Path(\"../data/raw/\")\n",
    "DATA_EXTRACTED = Path(\"../data/extracted/\")\n",
    "DATA_CSV = Path(\"../data/csv/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(source_dir: Path, target_dir: Path) -> bool:\n",
    "    if 0 != subprocess.call(f\"for f in {source_dir}/*.tar.gz; do tar -xvf \\\"$f\\\" -C {target_dir}; done;\",\n",
    "                            shell=True):\n",
    "        print(\"Error extracting tar files\", file=sys.stderr)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_batch_dir(batch_dir: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the batch directory contains all the necessary files.\n",
    "    \"\"\"\n",
    "\n",
    "    # eg for juicepsa-pds4-PI-01-juice_rad-20240417T191059\n",
    "    #                                      ^-------^\n",
    "    #                                               ^----^      \n",
    "    ts0 = batch_dir.name[-15:-7]\n",
    "    ts1 = batch_dir.name[-6:]\n",
    "\n",
    "    paths_valid = [\n",
    "        Path(f\"juicepsa-pds4-PI-01-juice_rad-{ts0}T{ts1}-checksum_manifest.tab\"),\n",
    "        Path(f\"juicepsa-pds4-PI-01-juice_rad-{ts0}T{ts1}-transfer_manifest.tab\"),\n",
    "        Path(f\"juicepsa-pds4-PI-01-juice_rad-{ts0}T{ts1}.xml\"),\n",
    "        Path(f\"juice_rad/data/raw/rad_raw_sc_{ts0}.cdf\"),\n",
    "        Path(f\"juice_rad/data/raw/rad_raw_sc_{ts0}.lblx\"),\n",
    "    ]\n",
    "\n",
    "    is_ok = True\n",
    "    for path in paths_valid:\n",
    "        if not batch_dir.joinpath(path).exists():\n",
    "            print(f\"Missing {path}\", file=sys.stderr)\n",
    "            is_ok = False\n",
    "    \n",
    "    return is_ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "!./0_fetching_ftp.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "extract_data(DATA_RAW, DATA_EXTRACTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "# for batch_dir in DATA_EXTRACTED.iterdir():\n",
    "#     if not batch_dir.is_dir():\n",
    "#         continue\n",
    "#     check_batch_dir(DATA_EXTRACTED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDF Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CDF data\n",
    "\n",
    "Output: `pycdf.CDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_path_science_cdf(path: Path) -> bool:\n",
    "    return path.name.startswith(\"rad_raw_sc_\") and path.name.endswith(\".cdf\")\n",
    "\n",
    "def is_path_housekeeping_cdf(path: Path) -> bool:\n",
    "    return path.name.startswith(\"rad_raw_hk_\") and path.name.endswith(\".cdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cdf(cdf_path: Path) -> pycdf.CDF:\n",
    "    \"\"\"\n",
    "    Note: It keeps the CDF file open, so it should be closed after use.\n",
    "    \"\"\"\n",
    "    return pycdf.CDF(str(cdf_path))\n",
    "\n",
    "    # cdf = None\n",
    "    # with pycdf.CDF(str(cdf_path)) as cdf:\n",
    "        # cdf = cdf.copy()\n",
    "    # return cdf\n",
    "\n",
    "def read_science_cdfs(data_dir: Path) -> List[pycdf.CDF]:\n",
    "    cdfs = []\n",
    "\n",
    "    for batch_dir in sorted(data_dir.iterdir()): \n",
    "        cdf_dir = batch_dir.joinpath(\"juice_rad/data_raw\") \n",
    "        for cdf_path in cdf_dir.glob(\"*.cdf\"):\n",
    "            if is_path_science_cdf(cdf_path):\n",
    "                cdfs.append(read_cdf(cdf_path))\n",
    "    return cdfs\n",
    "\n",
    "def read_housekeeping_cdfs(data_dir: Path) -> List[pycdf.CDF]:\n",
    "    cdfs = []\n",
    "\n",
    "    for batch_dir in sorted(data_dir.iterdir()): \n",
    "        cdf_dir = batch_dir.joinpath(\"juice_rad/data_raw\") \n",
    "        for cdf_path in cdf_dir.glob(\"*.cdf\"):\n",
    "            if is_path_housekeeping_cdf(cdf_path):\n",
    "                cdfs.append(read_cdf(cdf_path))\n",
    "    return cdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating CDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_science_cdfs(science_cdfs: List[pycdf.CDF]) -> bool:\n",
    "    # ...\n",
    "    # ...\n",
    "    # return all([19 == len(cdf.keys()) for cdf in science_cdfs])\n",
    "    return True\n",
    "\n",
    "def check_housekeeping_cdfs(housekeeping_cdfs: List[pycdf.CDF]) -> bool:\n",
    "    # ...\n",
    "    # ...\n",
    "    # return all([55 == cdf.keys() for cdf in housekeeping_cdfs])\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring CDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cdf_report(cdf: pycdf.CDF):\n",
    "    print(f'Keys:')\n",
    "    print(cdf)\n",
    "\n",
    "    print(f'\\nCDF meta:')\n",
    "    print(cdf.meta)\n",
    "    for key, val in cdf.items(): \n",
    "        print(f'\\n{key} -> {val}')\n",
    "        print(val.meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV/DataFrame processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting CDF to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_to_df(cdf: pycdf.CDF) -> pd.DataFrame:\n",
    "    df = pd.DataFrame((cdf[key][...] for key in cdf.keys())).T\n",
    "    df.columns = cdf.keys()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading / Saving CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV file into a pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    return df\n",
    "\n",
    "def save_csv(df: pd.DataFrame, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate a name and save a pandas DataFrame to a CSV file\n",
    "    \"\"\"\n",
    "    latest_time = max(df['time']).strftime('%Y%m%dT%H%M%S')\n",
    "    filename = DATA_CSV.joinpath(name + \"_\" + latest_time + '.csv')\n",
    "\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing DataFrame data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_df_time(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the time column to datetime and floor it to seconds, in place.\n",
    "    \"\"\"\n",
    "    df[\"time\"] = pd.to_datetime(df['time']).dt.floor('S')\n",
    "\n",
    "    return df\n",
    "\n",
    "def fix_df_time_start(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter the dataframe to only include events after September 1, 2023, in place.\n",
    "    \"\"\"\n",
    "    df.query(\"time >= '2023-09-01'\", inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def fix_df_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find and remove duplicates from the dataframe, in place.\n",
    "    \"\"\"\n",
    "    df.drop_duplicates(inplace=True, keep=\"first\")\n",
    "    return df\n",
    "\n",
    "def fix_sorting_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sort the dataframe by time, in place.\n",
    "    \"\"\"\n",
    "    df.sort_values(\"time\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def fix_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fix the dataframe in place.\n",
    "    \"\"\"\n",
    "    fix_df_time(df)\n",
    "    fix_df_time_start(df)\n",
    "    fix_df_duplicates(df)\n",
    "    fix_sorting_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating DataFrame data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_df_sorted(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Verify that the dataframe is sorted by \"time\"\n",
    "    \"\"\"\n",
    "    # Find rows where the 'time' is decreasing from the previous row\n",
    "    not_sorted_mask = df['time'].diff().dt.total_seconds() < 0\n",
    "\n",
    "    # The first row can't be \"not sorted\" by definition, so we can exclude it from the mask\n",
    "    not_sorted_mask.iloc[0] = False\n",
    "\n",
    "    # Filter the DataFrame to find the not sorted rows\n",
    "    not_sorted_rows = df[not_sorted_mask]\n",
    "\n",
    "    if not df['time'].is_monotonic_increasing:\n",
    "        raise ValueError(f\"Dataframe is not sorted by time:\\n{not_sorted_rows}\")\n",
    "\n",
    "def verify_df_time_diffs(df: pd.DataFrame, \n",
    "                         max_diff_tolerance: np.timedelta64 = np.timedelta64(90, 's'), \n",
    "                         min_diff_tolerance: np.timedelta64 = np.timedelta64(500, 'ms')) -> None:\n",
    "    \"\"\"\n",
    "    Verify that the time differences between events are within tolerance.\n",
    "    If time diff >= max_diff_tolerance, just prints the warning (data holes are permitted).\n",
    "    If time diff <= min_diff_tolerance, raises an exception (possible floating point errors).\n",
    "    \n",
    "    Assumes that the dataframe is non-decreasingly sorted by \"time\".  \n",
    "    \n",
    "    There may me multiple groups of events with the same time.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): input dataframe with \"time\" column\n",
    "        max_diff_tolerance (np.timedelta64, optional): max time difference tolerance in ms (warning only)\n",
    "        min_diff_tolerance (np.timedelta64, optional): min time difference tolerance in ms (exception)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: when time differences < min_diff_tolerance (possible floating point errors)\n",
    "    \"\"\"\n",
    "\n",
    "    # get all unique \"time\" values in df\n",
    "    times = df['time'].unique()\n",
    "\n",
    "    # calc time diffs\n",
    "    time_diffs = np.diff(times)\n",
    "\n",
    "    # check if all time diffs are not larger than the tolerance\n",
    "    checks = max_diff_tolerance > time_diffs\n",
    "    if not all(checks):\n",
    "        # find all indexes of unmet conditions\n",
    "        indexes = np.where(checks == False)[0]\n",
    "\n",
    "        # create a dataframe of times\n",
    "        df_times = pd.DataFrame(times, columns=[\"time\"])\n",
    "\n",
    "        # find all holes\n",
    "        holes = [f\"{df_times.iloc[i]['time']} and {df_times.iloc[i + 1]['time']}\" for i in indexes]\n",
    "        \n",
    "        print(\"Found time holes out of tolerance at times:\", *holes, sep='\\n\\t')\n",
    "\n",
    "\n",
    "    # check if all time diffs are not smaller than the tolerance\n",
    "    # (possible floating point errors)\n",
    "    checks = min_diff_tolerance < time_diffs\n",
    "    if not all(checks):\n",
    "        # find all indexes of unmet conditions\n",
    "        indexes = np.where(checks == False)[0]\n",
    "\n",
    "        # create a dataframe of times\n",
    "        df_times = pd.DataFrame(times, columns=[\"time\"])\n",
    "\n",
    "        # find all too close values\n",
    "        too_close = [f\"{df_times.iloc[i]['time']} and {df_times.iloc[i + 1]['time']}\" for i in indexes]\n",
    "        \n",
    "        raise ValueError(\n",
    "            \"Found time values too close to each other at times \" +\n",
    "            \"(possible floating point errors):\\n\\t\" +\n",
    "            \"\\n\\t\".join(too_close))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesing data categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature data conversions\n",
    "According to *RADEM User Manual*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hk_temp(adc_out: int | np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Convert housekeeping temperature (ADC output) to Celsius.\n",
    "\n",
    "    Notes:\n",
    "    - Uses Equation 6 for RADEM EQM/PFM HK from RADEM User Manual.\n",
    "    - Applicable for temperature sensors 1-5.\n",
    "    - 1 Celsius degree precision.\n",
    "    - RADEM operating range: -40 to +85 Celsius degrees.\n",
    "    \"\"\"\n",
    "    return np.round(adc_out * (3.3 / 4096) * (1000000 / 2210) - 273.16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reading CDFs\n",
    "hk_cdfs = read_housekeeping_cdfs(DATA_EXTRACTED)\n",
    "print(\"Found hk CDFs:\", len(hk_cdfs))\n",
    "print(f\"Checking hk CDFs: {check_housekeeping_cdfs(hk_cdfs)}\")\n",
    "\n",
    "# 2. Converting CDFs to DataFrames\n",
    "temp_cdf_keys = [\n",
    "    \"HK_Temp1_CEU\",\n",
    "    \"HK_PandI_Stack_Temp2\",\n",
    "    \"HK_E_Stack_Temp3\",\n",
    "    \"HK_DD_Temp4\",\n",
    "    \"HK_Temp5_CPU\",\n",
    "]\n",
    "\n",
    "temp_csv_keys = [\n",
    "    \"time\",\n",
    "    \"CEU Temperature (1)\",\n",
    "    \"P&IDH Temperature (2)\",\n",
    "    \"EDH Temperature (3)\",\n",
    "    \"DDH Temperature (4)\",\n",
    "    \"PCU Temperature (5)\",\n",
    "]\n",
    "\n",
    "hk_dfs = []\n",
    "for cdf in hk_cdfs:\n",
    "    df = pd.DataFrame(\n",
    "        np.vstack([\n",
    "            cdf[\"TIME_UTC\"][...], \n",
    "            *[convert_hk_temp(cdf[k][...]) for k in temp_cdf_keys]]).T, \n",
    "        columns=temp_csv_keys\n",
    "    )\n",
    "    hk_dfs.append(df)\n",
    "\n",
    "# close opened files\n",
    "del hk_cdfs\n",
    "\n",
    "# 3. Combining and fixing DataFrames\n",
    "df = pd.concat(hk_dfs)\n",
    "print(\"DF length before fixing:\", len(df))\n",
    "fix_df(df)\n",
    "print(\"DF length after fixing:\", len(df))\n",
    "\n",
    "# 4. Validating DataFrames (to be 100% sure)\n",
    "print(\"Verifying sorting\")\n",
    "verify_df_sorted(df)\n",
    "print(\"Verifying time diffs\")\n",
    "verify_df_time_diffs(df)\n",
    "\n",
    "# 5. Saving DataFrames to CSV\n",
    "save_csv(df, \"temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particles data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_particles(cdf: pycdf.CDF,\n",
    "                      cdf_particle_key: str, \n",
    "                      cdf_particle_bin: str) -> pd.DataFrame:\n",
    "    times = cdf[\"TIME_UTC\"][...]\n",
    "    particles = cdf[cdf_particle_key][...]\n",
    "    particle_bins = cdf[cdf_particle_bin][...]\n",
    "\n",
    "    time_col = np.repeat(times, len(particle_bins))\n",
    "    # event_type_col = np.full(len(particles) * len(particle_bins), event_type, dtype=\"U1\")\n",
    "    bin_col = np.tile(particle_bins, len(particles))\n",
    "    value_col = particles.flatten()\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"time\": time_col,\n",
    "        # \"event_type\": event_type_col,\n",
    "        \"bin\": bin_col,\n",
    "        \"value\": value_col\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional particles data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_df_time_p_counts(df: pd.DataFrame) -> None:\n",
    "    check = all(df[\"time\"].value_counts() == 9)\n",
    "\n",
    "    if not check:\n",
    "        raise ValueError(\"Incorrect number of proton events for some times\")\n",
    "\n",
    "\n",
    "def verify_df_time_e_counts(df: pd.DataFrame) -> None:\n",
    "    check = all(df[\"time\"].value_counts() == 9)\n",
    "    \n",
    "    if not check:\n",
    "        raise ValueError(\"Incorrect number of electron events for some times\")\n",
    "\n",
    "\n",
    "def verify_df_time_dd_counts(df: pd.DataFrame) -> None:\n",
    "    check = all(df[\"time\"].value_counts() == 31)\n",
    "    \n",
    "    if not check:\n",
    "        raise ValueError(\"Incorrect number of DD events for some times\")\n",
    "\n",
    "\n",
    "def verify_df_time_hi_counts(df: pd.DataFrame) -> None:\n",
    "    check = all(df[\"time\"].value_counts() == 8)\n",
    "    \n",
    "    if not check:\n",
    "        raise ValueError(\"Incorrect number of heavy ion events for some times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reading CDFs\n",
    "sc_cdfs = read_science_cdfs(DATA_EXTRACTED)\n",
    "\n",
    "print(\"Found sc CDFs:\", len(sc_cdfs))\n",
    "print(f\"Checking sc CDFs: {check_science_cdfs(sc_cdfs)}\")\n",
    "\n",
    "# 2. Converting CDFs to DataFrames\n",
    "df_p = pd.concat((\n",
    "    process_particles(cdf, \"PROTONS\", \"PROTON_BINS\")\n",
    "    for cdf in sc_cdfs\n",
    "))\n",
    "\n",
    "df_e = pd.concat((\n",
    "    process_particles(cdf, \"ELECTRONS\", \"ELECTRON_BINS\")\n",
    "    for cdf in sc_cdfs\n",
    "))\n",
    "\n",
    "df_d = pd.concat((\n",
    "    process_particles(cdf, \"DD\", \"DD_BINS\")\n",
    "    for cdf in sc_cdfs\n",
    "))\n",
    "\n",
    "df_h = pd.concat((\n",
    "    process_particles(cdf, \"HI_IONS\", \"HI_ION_BINS\")\n",
    "    for cdf in sc_cdfs\n",
    "))\n",
    "\n",
    "print(\"Proton measurements:\", len(df_p))\n",
    "print(\"Electron measurements:\", len(df_e))\n",
    "print(\"DD measurements:\", len(df_d))\n",
    "print(\"Heavy ion measurements:\", len(df_h))\n",
    "\n",
    "# close opened files\n",
    "del sc_cdfs\n",
    "\n",
    "# 3. Fixing DataFrames\n",
    "fix_df(df_p)\n",
    "fix_df(df_e)\n",
    "fix_df(df_d)\n",
    "fix_df(df_h)\n",
    "\n",
    "print(\"Proton measurements:\", len(df_p), \"(after fixing)\")\n",
    "print(\"Electron measurements:\", len(df_e), \"(after fixing)\")\n",
    "print(\"DD measurements:\", len(df_d), \"(after fixing)\")\n",
    "print(\"Heavy ion measurements:\", len(df_h), \"(after fixing)\")\n",
    "\n",
    "# 4. Validating DataFrames\n",
    "print(\"Verifying sorting\")\n",
    "verify_df_sorted(df_p)\n",
    "verify_df_sorted(df_e)\n",
    "verify_df_sorted(df_d)\n",
    "verify_df_sorted(df_h)\n",
    "print(\"Verifying time diffs\")\n",
    "verify_df_time_diffs(df_p)\n",
    "verify_df_time_diffs(df_e)\n",
    "verify_df_time_diffs(df_d)\n",
    "verify_df_time_diffs(df_h)\n",
    "print(\"Verifying time counts\")\n",
    "verify_df_time_p_counts(df_p)\n",
    "verify_df_time_e_counts(df_e)\n",
    "verify_df_time_dd_counts(df_d)\n",
    "verify_df_time_hi_counts(df_h)\n",
    "\n",
    "# 5. Saving DataFrames to CSV\n",
    "print(\"Saving CSVs\")\n",
    "save_csv(df_p, \"protons\")\n",
    "save_csv(df_e, \"electrons\")\n",
    "save_csv(df_d, \"dd\")\n",
    "save_csv(df_h, \"heavy_ions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patricle Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reading CDFs\n",
    "sc_cdfs = read_science_cdfs(DATA_EXTRACTED)\n",
    "\n",
    "print(\"Found sc CDFs:\", len(sc_cdfs))\n",
    "print(f\"Checking sc CDFs: {check_science_cdfs(sc_cdfs)}\")\n",
    "\n",
    "# 2. Converting CDFs to DataFrames\n",
    "cdf = sc_cdfs[0]\n",
    "flux_labels = cdf[\"LABEL_FLUX\"][...]\n",
    "print(flux_labels)\n",
    "\n",
    "dfs = {}\n",
    "for idx, flux_label in enumerate(flux_labels):\n",
    "    dfs[flux_label] = pd.concat((\n",
    "        pd.DataFrame({\n",
    "            \"time\": cdf[\"TIME_UTC\"][...],\n",
    "            \"value\": cdf[\"FLUX\"][...][:, idx]\n",
    "        })\n",
    "        for cdf in sc_cdfs\n",
    "    ))\n",
    "\n",
    "# close opened files\n",
    "del sc_cdfs\n",
    "\n",
    "# 3. Fixing DataFrames\n",
    "for df in dfs.values():\n",
    "    fix_df(df)\n",
    "\n",
    "# 4. Validating DataFrames (to be 100% sure)\n",
    "for df in dfs.values():\n",
    "    verify_df_sorted(df)\n",
    "    verify_df_time_diffs(df)\n",
    "\n",
    "# 5. Saving DataFrames to CSV\n",
    "save_csv(dfs[\"PIDH\"], \"protons_flux\")\n",
    "save_csv(dfs[\"EDH \"], \"electrons_flux\") # NOTE: \"EDH \" is correct, not \"EDH\"\n",
    "save_csv(dfs[\"DDH \"], \"dd_flux\") # NOTE: \"DDH \" is correct, not \"DDH\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

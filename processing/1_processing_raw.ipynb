{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1: Processing raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacepy `CDF_LIB` check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get('CDF_LIB', '') == '':\n",
    "    print('No CDF_LIB environment variable found for CDF file processing.')\n",
    "from spacepy import pycdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW = Path(\"../data_raw/\")\n",
    "DATA_EXTRACTED = Path(\"../data_extracted/\")\n",
    "DATA_CSV = Path(\"../data_csv/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fetching data\n",
    "TBI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(source_dir: Path, target_dir: Path) -> bool:\n",
    "    if 0 != subprocess.call(f\"for f in {source_dir}/*.tar.gz; do tar -xvf \\\"$f\\\" -C {target_dir}; done;\",\n",
    "                            shell=True):\n",
    "        print(\"Error extracting tar files\", file=sys.stderr)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracted data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_batch_dir(batch_dir: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the batch directory contains all the necessary files.\n",
    "    \"\"\"\n",
    "\n",
    "    # eg for juicepsa-pds4-PI-01-juice_rad-20240417T191059\n",
    "    #                                      ^-------^\n",
    "    #                                               ^----^      \n",
    "    ts0 = batch_dir.name[-15:-7]\n",
    "    ts1 = batch_dir.name[-6:]\n",
    "\n",
    "    paths_valid = [\n",
    "        Path(f\"juicepsa-pds4-PI-01-juice_rad-{ts0}T{ts1}-checksum_manifest.tab\"),\n",
    "        Path(f\"juicepsa-pds4-PI-01-juice_rad-{ts0}T{ts1}-transfer_manifest.tab\"),\n",
    "        Path(f\"juicepsa-pds4-PI-01-juice_rad-{ts0}T{ts1}.xml\"),\n",
    "        Path(f\"juice_rad/data_raw/rad_raw_sc_{ts0}.cdf\"),\n",
    "        Path(f\"juice_rad/data_raw/rad_raw_sc_{ts0}.lblx\"),\n",
    "    ]\n",
    "\n",
    "    is_ok = True\n",
    "    for path in paths_valid:\n",
    "        if not batch_dir.joinpath(path).exists():\n",
    "            print(f\"Missing {path}\", file=sys.stderr)\n",
    "            is_ok = False\n",
    "    \n",
    "    return is_ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_path_science_cdf(path: Path) -> bool:\n",
    "    return path.name.startswith(\"rad_raw_sc_\") and path.name.endswith(\".cdf\")\n",
    "\n",
    "def is_path_housekeeping_cdf(path: Path) -> bool:\n",
    "    return path.name.startswith(\"rad_raw_hk_\") and path.name.endswith(\".cdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cdf(cdf_path: Path) -> pd.DataFrame:\n",
    "    return pycdf.CDF(str(cdf_path))\n",
    "\n",
    "    # cdf = None\n",
    "    # with pycdf.CDF(str(cdf_path)) as cdf:\n",
    "        # cdf = cdf.copy()\n",
    "    # return cdf\n",
    "\n",
    "def read_science_cdfs(data_dir: Path) -> List[pycdf.CDF]:\n",
    "    cdfs = []\n",
    "\n",
    "    for batch_dir in sorted(data_dir.iterdir()): \n",
    "        cdf_dir = batch_dir.joinpath(\"juice_rad/data_raw\") \n",
    "        for cdf_path in cdf_dir.glob(\"*.cdf\"):\n",
    "            if is_path_science_cdf(cdf_path):\n",
    "                cdfs.append(read_cdf(cdf_path))\n",
    "    return cdfs\n",
    "\n",
    "def read_housekeeping_cdfs(data_dir: Path) -> List[pycdf.CDF]:\n",
    "    cdfs = []\n",
    "\n",
    "    for batch_dir in sorted(data_dir.iterdir()): \n",
    "        cdf_dir = batch_dir.joinpath(\"juice_rad/data_raw\") \n",
    "        for cdf_path in cdf_dir.glob(\"*.cdf\"):\n",
    "            if is_path_housekeeping_cdf(cdf_path):\n",
    "                cdfs.append(read_cdf(cdf_path))\n",
    "    return cdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_science_cdfs(science_cdfs: List[pycdf.CDF]) -> bool:\n",
    "    # ...\n",
    "    # ...\n",
    "    return all([19 == len(cdf.keys()) for cdf in science_cdfs])\n",
    "\n",
    "def check_housekeeping_cdfs(housekeeping_cdfs: List[pycdf.CDF]) -> bool:\n",
    "    # ...\n",
    "    # ...\n",
    "    return all([55 == cdf.keys() for cdf in housekeeping_cdfs])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cdf_report(cdf: pycdf.CDF):\n",
    "    print(f'Keys:')\n",
    "    print(cdf)\n",
    "\n",
    "    print(f'\\nCDF meta:')\n",
    "    print(cdf.meta)\n",
    "    for key, val in cdf.items(): \n",
    "        print(f'\\n{key} -> {val}')\n",
    "        print(val.meta)\n",
    "\n",
    "def hk_cdf_to_raw_df(cdf: pycdf.CDF) -> pd.DataFrame:\n",
    "    df = pd.DataFrame((cdf[key][...] for key in cdf.keys())).T\n",
    "    df.columns = cdf.keys()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data conversions\n",
    "According to *RADEM User Manual*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hk_temp(adc_out: int | np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Convert housekeeping temperature (ADC output) to Celsius.\n",
    "\n",
    "    Notes:\n",
    "    - Uses Equation 6 for RADEM EQM/PFM HK from RADEM User Manual.\n",
    "    - Applicable for temperature sensors 1-5.\n",
    "    - 1 Celsius degree precision.\n",
    "    - RADEM operating range: -40 to +85 Celsius degrees.\n",
    "    \"\"\"\n",
    "    return np.round(adc_out * (3.3 / 4096) * (1000000 / 2210) - 273.16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage & examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = DATA_EXTRACTED.joinpath(\"juicepsa-pds4-PI-01-juice_rad-20240417T192313/juice_rad/data_raw/rad_raw_hk_20240417.cdf\")\n",
    "cdf = read_cdf(path)\n",
    "print_cdf_report(cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_cdfs = read_housekeeping_cdfs(DATA_EXTRACTED)\n",
    "science_cdfs = read_science_cdfs(DATA_EXTRACTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_housekeeping_cdfs(hk_cdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_science_cdfs(science_cdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree ../data_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_df_time(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the time column to datetime and floor it to seconds, in place.\n",
    "    \"\"\"\n",
    "    df[\"time\"] = pd.to_datetime(df['time']).dt.floor('S')\n",
    "\n",
    "    return df\n",
    "\n",
    "def fix_df_time_start(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter the dataframe to only include events after September 1, 2023, in place.\n",
    "    \"\"\"\n",
    "    df.query(\"time >= '2023-09-01'\", inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def fix_df_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find and remove duplicates from the dataframe, in place.\n",
    "    \"\"\"\n",
    "    df.drop_duplicates(inplace=True, keep=\"first\")\n",
    "    return df\n",
    "\n",
    "def fix_sorting_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sort the dataframe by time, in place.\n",
    "    \"\"\"\n",
    "    df.sort_values(\"time\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def fix_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fix the dataframe in place.\n",
    "    \"\"\"\n",
    "    fix_df_time(df)\n",
    "    fix_df_time_start(df)\n",
    "    fix_df_duplicates(df)\n",
    "    fix_sorting_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_df_sorted(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Verify that the dataframe is sorted by \"time\"\n",
    "    \"\"\"\n",
    "    # Find rows where the 'time' is decreasing from the previous row\n",
    "    not_sorted_mask = df['time'].diff().dt.total_seconds() < 0\n",
    "\n",
    "    # The first row can't be \"not sorted\" by definition, so we can exclude it from the mask\n",
    "    not_sorted_mask.iloc[0] = False\n",
    "\n",
    "    # Filter the DataFrame to find the not sorted rows\n",
    "    not_sorted_rows = df[not_sorted_mask]\n",
    "\n",
    "    if not df['time'].is_monotonic_increasing:\n",
    "        raise ValueError(f\"Dataframe is not sorted by time:\\n{not_sorted_rows}\")\n",
    "\n",
    "def verify_df_time_diffs(df: pd.DataFrame, \n",
    "                         max_diff_tolerance: np.timedelta64 = np.timedelta64(90, 's'), \n",
    "                         min_diff_tolerance: np.timedelta64 = np.timedelta64(500, 'ms')) -> None:\n",
    "    \"\"\"\n",
    "    Verify that the time differences between events are within tolerance.\n",
    "    If time diff >= max_diff_tolerance, just prints the warning (data holes are permitted).\n",
    "    If time diff <= min_diff_tolerance, raises an exception (possible floating point errors).\n",
    "    \n",
    "    Assumes that the dataframe is non-decreasingly sorted by \"time\".  \n",
    "    \n",
    "    There may me multiple groups of events with the same time.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): input dataframe with \"time\" column\n",
    "        max_diff_tolerance (np.timedelta64, optional): max time difference tolerance in ms (warning only)\n",
    "        min_diff_tolerance (np.timedelta64, optional): min time difference tolerance in ms (exception)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: when time differences < min_diff_tolerance (possible floating point errors)\n",
    "    \"\"\"\n",
    "\n",
    "    # get all unique \"time\" values in df\n",
    "    times = df['time'].unique()\n",
    "\n",
    "    # calc time diffs\n",
    "    time_diffs = np.diff(times)\n",
    "\n",
    "    # check if all time diffs are not larger than the tolerance\n",
    "    checks = max_diff_tolerance > time_diffs\n",
    "    if not all(checks):\n",
    "        # find all indexes of unmet conditions\n",
    "        indexes = np.where(checks == False)[0]\n",
    "\n",
    "        # create a dataframe of times\n",
    "        df_times = pd.DataFrame(times, columns=[\"time\"])\n",
    "\n",
    "        # find all holes\n",
    "        holes = [f\"{df_times.iloc[i]['time']} and {df_times.iloc[i + 1]['time']}\" for i in indexes]\n",
    "        \n",
    "        print(\"Found time holes out of tolerance at times:\", *holes, sep='\\n\\t')\n",
    "\n",
    "\n",
    "    # check if all time diffs are not smaller than the tolerance\n",
    "    # (possible floating point errors)\n",
    "    checks = min_diff_tolerance < time_diffs\n",
    "    if not all(checks):\n",
    "        # find all indexes of unmet conditions\n",
    "        indexes = np.where(checks == False)[0]\n",
    "\n",
    "        # create a dataframe of times\n",
    "        df_times = pd.DataFrame(times, columns=[\"time\"])\n",
    "\n",
    "        # find all too close values\n",
    "        too_close = [f\"{df_times.iloc[i]['time']} and {df_times.iloc[i + 1]['time']}\" for i in indexes]\n",
    "        \n",
    "        raise ValueError(\n",
    "            \"Found time values too close to each other at times \" +\n",
    "            \"(possible floating point errors):\\n\\t\" +\n",
    "            \"\\n\\t\".join(too_close))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = read_cdf(DATA_EXTRACTED.joinpath(\"juicepsa-pds4-PI-01-juice_rad-20240202T182054/juice_rad/data_raw/rad_raw_hk_20240202.cdf\"))\n",
    "\n",
    "temp_keys = [\n",
    "    \"HK_Temp1_CEU\",\n",
    "    \"HK_PandI_Stack_Temp2\",\n",
    "    \"HK_E_Stack_Temp3\",\n",
    "    \"HK_DD_Temp4\",\n",
    "    \"HK_Temp5_CPU\",\n",
    "]\n",
    "\n",
    "alt_keys = [\n",
    "    \"time\",\n",
    "    \"CEU Temperature (1)\",\n",
    "    \"P&IDH Temperature (2)\",\n",
    "    \"EDH Temperature (3)\",\n",
    "    \"DDH Temperature (4)\",\n",
    "    \"PCU Temperature (5)\",\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    np.vstack([cdf[\"TIME_UTC\"][...], *[convert_hk_temp(cdf[k][...]) for k in temp_keys]]).T, \n",
    "    columns=alt_keys)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdfs = read_housekeeping_cdfs(DATA_EXTRACTED)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for cdf in cdfs:\n",
    "    df = pd.DataFrame(\n",
    "        np.vstack([\n",
    "            cdf[\"TIME_UTC\"][...], \n",
    "            *[convert_hk_temp(cdf[k][...]) for k in temp_keys]]).T, \n",
    "        columns=alt_keys\n",
    "    )\n",
    "    dfs.append(df)\n",
    "\n",
    "# del cdfs\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "print(\"DF length before fixing:\", len(df))\n",
    "fix_df(df)\n",
    "print(\"DF length after fixing:\", len(df))\n",
    "\n",
    "# for row in df.to_dict(orient=\"records\"):\n",
    "#     print(row)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_df_sorted(df)\n",
    "verify_df_time_diffs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_CSV.joinpath(\"hk_temp.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

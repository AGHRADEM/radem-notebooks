{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 3: Processing Line Protocol & uploading to InfluxDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "1. [InfluxDB installed](https://www.influxdata.com/downloads/).\n",
    "2. Export InfluxDB API Key in `.env` file.\n",
    "3. Prepare preprocessed CSV data using previous notebook or other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import influxdb_client\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "os.makedirs(\"../data/line_protocol\", exist_ok=True)\n",
    "\n",
    "DATA_PREROCESSED_DIR = Path(\"../data/csv\")\n",
    "DATA_LINE_PROTOCOL_DIR = Path(\"../data/line_protocol\")\n",
    "\n",
    "TOKEN = os.environ.get(\"INFLUXDB_TOKEN\")\n",
    "URL = \"http://localhost:8086\"\n",
    "ORG = \"radem\"\n",
    "\n",
    "BUCKET = \"radem\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the InfluxDB connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_write_api(url: str = URL, token: str = TOKEN, org: str = ORG):\n",
    "    client = influxdb_client.InfluxDBClient(\n",
    "        url=url,\n",
    "        token=token,\n",
    "        org=org\n",
    "    )\n",
    "\n",
    "    write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "    return write_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove & Clear InfluxDB bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buckets_api(url: str = URL, token: str = TOKEN, org: str = ORG):\n",
    "    client = influxdb_client.InfluxDBClient(\n",
    "        url=url,\n",
    "        token=token,\n",
    "        org=org\n",
    "    )\n",
    "\n",
    "    buckets_api = client.buckets_api()\n",
    "\n",
    "    return buckets_api\n",
    "\n",
    "\n",
    "def create_bucket(buckets_api, bucket_name: str, org: str):\n",
    "    client = influxdb_client.InfluxDBClient(\n",
    "        url=URL,\n",
    "        token=TOKEN,\n",
    "        org=ORG\n",
    "    )\n",
    "\n",
    "    buckets_api = client.buckets_api()\n",
    "\n",
    "    bucket = buckets_api.create_bucket(bucket_name=bucket_name, org=org)\n",
    "\n",
    "    return bucket\n",
    "\n",
    "def find_bucket_by_name(buckets_api, bucket_name):\n",
    "    buckets = buckets_api.find_buckets().buckets\n",
    "    for bucket in buckets:\n",
    "        if bucket.name == bucket_name:\n",
    "            return bucket\n",
    "    return None\n",
    "\n",
    "def delete_bucket(buckets_api, bucket_name):\n",
    "    bucket = find_bucket_by_name(buckets_api, bucket_name)\n",
    "    if bucket:\n",
    "        buckets_api.delete_bucket(bucket)\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading preprocessed CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_temperature(filename: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(str(filename))\n",
    "\n",
    "    # Convert time\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # Convert time to ns for InfluxDB\n",
    "    df['time_ns'] = pd.to_datetime(df['time']).astype('int64')\n",
    "\n",
    "    # Convert temperatures to int\n",
    "    df[\"CEU Temperature (1)\"] = df[\"CEU Temperature (1)\"].astype(\"int64\")\n",
    "    df[\"P&IDH Temperature (2)\"] = df[\"P&IDH Temperature (2)\"].astype(\"int64\")\n",
    "    df[\"EDH Temperature (3)\"] = df[\"EDH Temperature (3)\"].astype(\"int64\")\n",
    "    df[\"DDH Temperature (4)\"] = df[\"DDH Temperature (4)\"].astype(\"int64\")\n",
    "    df[\"PCU Temperature (5)\"] = df[\"PCU Temperature (5)\"].astype(\"int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_particles(filename: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(str(filename))\n",
    "\n",
    "    # Convert time\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # Convert time to ns for InfluxDB\n",
    "    df['time_ns'] = pd.to_datetime(df['time']).astype('int64')\n",
    "\n",
    "    # Converts\n",
    "    df[\"bin\"] = df[\"bin\"].astype(\"int8\")\n",
    "    df[\"value\"] = df[\"value\"].astype(\"int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_flux(filename: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(str(filename))\n",
    "\n",
    "    # Convert time\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # Convert time to ns for InfluxDB\n",
    "    df['time_ns'] = pd.to_datetime(df['time']).astype('int64')\n",
    "\n",
    "    # Converts\n",
    "    df[\"value\"] = df[\"value\"].astype(\"int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_distance(filename: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(str(filename))\n",
    "\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # Convert time to ns for InfluxDB\n",
    "    df['time_ns'] = pd.to_datetime(df['time']).astype('int64')\n",
    "\n",
    "    # Converts\n",
    "    df[\"dist\"] = df[\"dist\"].astype(\"float64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_angles(filename: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(str(filename))\n",
    "\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # Convert time to ns for InfluxDB\n",
    "    df['time_ns'] = pd.to_datetime(df['time']).astype('int64')\n",
    "\n",
    "    # Converts\n",
    "    df[\"ra\"] = df[\"ra\"].astype(\"float64\")\n",
    "    df[\"dec\"] = df[\"dec\"].astype(\"float64\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting DataFrame -> Line Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_temp_to_line_protocol(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    measurements = [\n",
    "        \"temp1_ceu\",\n",
    "        \"temp2_pidh\",\n",
    "        \"temp3_edh\",\n",
    "        \"temp4_ddh\",\n",
    "        \"temp5_pcu\"\n",
    "    ]\n",
    "\n",
    "    labels = [\n",
    "        \"CEU Temperature (1)\",\n",
    "        \"P&IDH Temperature (2)\",\n",
    "        \"EDH Temperature (3)\",\n",
    "        \"DDH Temperature (4)\",\n",
    "        \"PCU Temperature (5)\"\n",
    "    ]\n",
    "    \n",
    "    df = pd.concat((\n",
    "        pd.DataFrame(\n",
    "            measurement + \" \" + \n",
    "            \"value=\" + df[label].astype(str) + \"i \" + \n",
    "            df['time_ns'].astype(str),\n",
    "            columns=[\"line\"]\n",
    "        )\n",
    "        for measurement, label in zip(measurements, labels)\n",
    "    ), ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def convert_particles_to_line_protocol(df: pd.DataFrame, measurement_name: str) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(\n",
    "        measurement_name + \n",
    "        \",bin=\" + df[\"bin\"].astype(str) + \" \" \n",
    "        \"value=\" + df[\"value\"].astype(str) + \"i \" + \n",
    "        df['time_ns'].astype(str),\n",
    "        columns=[\"line\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def convert_flux_to_line_protocol(df: pd.DataFrame, measurement_name: str) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(\n",
    "        measurement_name + \n",
    "        \" \" + \n",
    "        \"value=\" + df[\"value\"].astype(str) + \"i \" + \n",
    "        df['time_ns'].astype(str),\n",
    "        columns=[\"line\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def convert_distance_to_line_protocol(df: pd.DataFrame, measurement_name: str) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(\n",
    "        measurement_name + \n",
    "        \" \" + \n",
    "        \"value=\" + df[\"dist\"].astype(str) + \" \" + \n",
    "        df['time_ns'].astype(str),\n",
    "        columns=[\"line\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def convert_angles_to_line_protocol(df: pd.DataFrame, measurement_name: str) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(\n",
    "        measurement_name + \n",
    "        \" \" + \n",
    "        \"ra=\" + df[\"ra\"].astype(str) + \",\" +\n",
    "        \"dec=\" + df[\"dec\"].astype(str) + \" \" +\n",
    "        df['time_ns'].astype(str),\n",
    "        columns=[\"line\"]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Line Protocol file\n",
    "\n",
    "Example line: `my_measurement,event_type=e,channel=0 value=123 1556813561098000000`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_line_protocol(df: pd.DataFrame, filename: Path):\n",
    "    df.to_csv(filename, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Line Protocol file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_line_protocol(filename: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        str(filename), \n",
    "        header=None, \n",
    "        sep='\\0', \n",
    "        names=['line']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_line_protocol(\n",
    "        write_api: influxdb_client.WriteApi, \n",
    "        df_lines: pd.DataFrame,\n",
    "        bucket: str,\n",
    "        org: str, \n",
    "        batch_size: int = 1000000) -> None:\n",
    "    for batch in range(0, len(df_lines), batch_size):\n",
    "        batch_end = min(batch + batch_size - 1, len(df_lines) - 1)\n",
    "        batch_indices = slice(batch, batch_end)\n",
    "\n",
    "        print(f\"Uploading batch of {batch_indices.stop - batch_indices.start + 1} records, from {batch_indices.start} to {batch_indices.stop}.\")\n",
    "\n",
    "        write_api.write(bucket, org, df_lines.loc[batch_indices, 'line'])\n",
    "\n",
    "    write_api.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buckets setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': datetime.datetime(2024, 9, 22, 23, 4, 4, 572752, tzinfo=tzutc()),\n",
       " 'description': None,\n",
       " 'id': '0469bc082570cf8e',\n",
       " 'labels': [],\n",
       " 'links': {'_self': '/api/v2/buckets/0469bc082570cf8e',\n",
       "           'labels': '/api/v2/buckets/0469bc082570cf8e/labels',\n",
       "           'members': '/api/v2/buckets/0469bc082570cf8e/members',\n",
       "           'org': '/api/v2/orgs/4344de8debf2b285',\n",
       "           'owners': '/api/v2/buckets/0469bc082570cf8e/owners',\n",
       "           'write': '/api/v2/write?org=4344de8debf2b285&bucket=0469bc082570cf8e'},\n",
       " 'name': 'radem',\n",
       " 'org_id': '4344de8debf2b285',\n",
       " 'retention_rules': [{'every_seconds': 0,\n",
       "                      'shard_group_duration_seconds': 604800,\n",
       "                      'type': 'expire'}],\n",
       " 'rp': '0',\n",
       " 'schema_type': None,\n",
       " 'type': 'user',\n",
       " 'updated_at': datetime.datetime(2024, 9, 22, 23, 4, 4, 572752, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buckets_api = get_buckets_api()\n",
    "\n",
    "delete_bucket(buckets_api, BUCKET)\n",
    "\n",
    "create_bucket(buckets_api, BUCKET, ORG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading batch of 2827 records, from 0 to 2826.\n"
     ]
    }
   ],
   "source": [
    "# 1. get the newest CSV filename\n",
    "csv_filename = sorted(DATA_PREROCESSED_DIR.glob(\"juice_distance_to_sun.csv\"))[-1]\n",
    "\n",
    "# 2. read the CSV file\n",
    "df = read_distance(csv_filename)\n",
    "\n",
    "# 3. convert the CSV file to line protocol\n",
    "df_lines = convert_distance_to_line_protocol(df, \"dist_sun\")\n",
    "\n",
    "# 4. save the line protocol to a file\n",
    "line_protocol_filename = DATA_LINE_PROTOCOL_DIR / f\"{csv_filename.stem}.txt\"\n",
    "save_line_protocol(df_lines, line_protocol_filename)\n",
    "\n",
    "# 5. upload the line protocol to InfluxDB\n",
    "write_api = get_write_api(\n",
    "    url=URL,\n",
    "    token=TOKEN,\n",
    "    org=ORG\n",
    ")\n",
    "\n",
    "upload_line_protocol(write_api, df_lines, BUCKET, ORG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading batch of 2827 records, from 0 to 2826.\n"
     ]
    }
   ],
   "source": [
    "# 1. get the newest CSV filename\n",
    "csv_filename = sorted(DATA_PREROCESSED_DIR.glob(\"juice_distance_to_earth.csv\"))[-1]\n",
    "\n",
    "# 2. read the CSV file\n",
    "df = read_distance(csv_filename)\n",
    "\n",
    "# 3. convert the CSV file to line protocol\n",
    "df_lines = convert_distance_to_line_protocol(df, \"dist_earth\")\n",
    "\n",
    "# 4. save the line protocol to a file\n",
    "line_protocol_filename = DATA_LINE_PROTOCOL_DIR / f\"{csv_filename.stem}.txt\"\n",
    "save_line_protocol(df_lines, line_protocol_filename)\n",
    "\n",
    "# 5. upload the line protocol to InfluxDB\n",
    "write_api = get_write_api(\n",
    "    url=URL,\n",
    "    token=TOKEN,\n",
    "    org=ORG\n",
    ")\n",
    "\n",
    "upload_line_protocol(write_api, df_lines, BUCKET, ORG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get the newest CSV filename\n",
    "csv_filename = sorted(DATA_PREROCESSED_DIR.glob(\"juice_angles_to_sun.csv\"))[-1]\n",
    "\n",
    "# 2. read the CSV file\n",
    "df = read_angles(csv_filename)\n",
    "\n",
    "# 3. convert the CSV file to line protocol\n",
    "df_lines = convert_angles_to_line_protocol(df, \"angles_sun\")\n",
    "\n",
    "# 4. save the line protocol to a file\n",
    "line_protocol_filename = DATA_LINE_PROTOCOL_DIR / f\"{csv_filename.stem}.txt\"\n",
    "save_line_protocol(df_lines, line_protocol_filename)\n",
    "\n",
    "# 5. upload the line protocol to InfluxDB\n",
    "# write_api = get_write_api(\n",
    "#     url=URL,\n",
    "#     token=TOKEN,\n",
    "#     org=ORG\n",
    "# )\n",
    "\n",
    "# upload_line_protocol(write_api, df_lines, BUCKET, ORG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get the newest CSV filename\n",
    "csv_filename = sorted(DATA_PREROCESSED_DIR.glob(\"temperature_*.csv\"))[-1]\n",
    "\n",
    "# 2. read the CSV file\n",
    "df = read_temperature(csv_filename)\n",
    "\n",
    "# 3. convert the CSV file to line protocol\n",
    "df_lines = convert_temp_to_line_protocol(df)\n",
    "\n",
    "# 4. (optional) save the line protocol to a file\n",
    "line_protocol_filename = DATA_LINE_PROTOCOL_DIR / f\"{csv_filename.stem}.line\"\n",
    "save_line_protocol(df_lines, line_protocol_filename)\n",
    "\n",
    "# 5. upload the line protocol to InfluxDB\n",
    "write_api = get_write_api(\n",
    "    url=URL, \n",
    "    token=TOKEN, \n",
    "    org=ORG)\n",
    "\n",
    "upload_line_protocol(\n",
    "    write_api=write_api, \n",
    "    df_lines=df_lines, \n",
    "    bucket=BUCKET, \n",
    "    org=ORG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for particle in [\"protons\", \"electrons\", \"dd\", \"heavy_ions\"]:\n",
    "    print(f\"Uploading {particle} data...\")\n",
    "\n",
    "    # 1. get the newest CSV filename\n",
    "    csv_filename = sorted(DATA_PREROCESSED_DIR.glob(f\"{particle}_2*.csv\"))[-1]\n",
    "\n",
    "    # 2. read the CSV file\n",
    "    df = read_particles(csv_filename)\n",
    "    print(f\"Read {len(df)} records from {csv_filename}\")\n",
    "\n",
    "    # 3. convert the CSV file to line protocol\n",
    "    df_lines = convert_particles_to_line_protocol(df, particle)\n",
    "\n",
    "    # 4. (optional) save the line protocol to a file\n",
    "    line_protocol_filename = DATA_LINE_PROTOCOL_DIR / f\"{csv_filename.stem}.line\"\n",
    "    save_line_protocol(df_lines, line_protocol_filename)\n",
    "\n",
    "    # 5. upload the line protocol to InfluxDB\n",
    "    write_api = get_write_api(\n",
    "        url=URL, \n",
    "        token=TOKEN, \n",
    "        org=ORG)\n",
    "\n",
    "    upload_line_protocol(\n",
    "        write_api=write_api, \n",
    "        df_lines=df_lines, \n",
    "        bucket=BUCKET, \n",
    "        org=ORG)\n",
    "    \n",
    "    write_api.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for particle in [\"protons_flux\", \"electrons_flux\", \"dd_flux\"]:\n",
    "    print(f\"Uploading {particle} data...\")\n",
    "\n",
    "    # 1. get the newest CSV filename\n",
    "    csv_filename = sorted(DATA_PREROCESSED_DIR.glob(f\"{particle}_*.csv\"))[-1]\n",
    "\n",
    "    # 2. read the CSV file\n",
    "    df = read_flux(csv_filename)\n",
    "    print(f\"Read {len(df)} records from {csv_filename}\")\n",
    "\n",
    "    # 3. convert the CSV file to line protocol\n",
    "    df_lines = convert_flux_to_line_protocol(df, particle)\n",
    "\n",
    "    # 4. (optional) save the line protocol to a file\n",
    "    line_protocol_filename = DATA_LINE_PROTOCOL_DIR / f\"{csv_filename.stem}.line\"\n",
    "    save_line_protocol(df_lines, line_protocol_filename)\n",
    "\n",
    "    # 5. upload the line protocol to InfluxDB\n",
    "    write_api = get_write_api(\n",
    "        url=URL, \n",
    "        token=TOKEN, \n",
    "        org=ORG)\n",
    "\n",
    "    upload_line_protocol(\n",
    "        write_api=write_api, \n",
    "        df_lines=df_lines, \n",
    "        bucket=BUCKET, \n",
    "        org=ORG)\n",
    "    \n",
    "    write_api.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

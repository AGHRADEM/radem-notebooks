{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 3: Processing Line Protocol & uploading to InfluxDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "1. [InfluxDB installed](https://www.influxdata.com/downloads/).\n",
    "2. Export InfluxDB API Key in `.env` file.\n",
    "3. Prepare preprocessed CSV data using previous notebook or other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../data_irem/line_protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import influxdb_client\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "\n",
    "DATA_PREROCESSED_DIR = Path(\"../data_irem/csv\")\n",
    "DATA_LINE_PROTOCOL_DIR = Path(\"../data_irem/line_protocol\")\n",
    "\n",
    "TOKEN = os.environ.get(\"INFLUXDB_TOKEN\")\n",
    "URL = \"http://localhost:8086\"\n",
    "ORG = \"radem\"\n",
    "\n",
    "BUCKET = \"radem\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the InfluxDB connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_write_api(url: str = URL, token: str = TOKEN, org: str = ORG):\n",
    "    client = influxdb_client.InfluxDBClient(\n",
    "        url=url,\n",
    "        token=token,\n",
    "        org=org\n",
    "    )\n",
    "\n",
    "    write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "    return write_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading preprocessed CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_particles(filename: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(str(filename))\n",
    "\n",
    "    # Convert time\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # Convert time to ns for InfluxDB\n",
    "    df['time_ns'] = pd.to_datetime(df['time']).astype('int64')\n",
    "\n",
    "    # Converts\n",
    "    df[\"bin\"] = df[\"bin\"].astype(\"int8\")\n",
    "    df[\"value\"] = df[\"value\"].astype(\"int64\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting DataFrame -> Line Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_particles_to_line_protocol(df: pd.DataFrame, measurement_name: str) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(\n",
    "        measurement_name + \n",
    "        \",bin=\" + df[\"bin\"].astype(str) + \" \" \n",
    "        \"value=\" + df[\"value\"].astype(str) + \"i \" + \n",
    "        df['time_ns'].astype(str),\n",
    "        columns=[\"line\"]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Line Protocol file\n",
    "\n",
    "Example line: `my_measurement,event_type=e,channel=0 value=123 1556813561098000000`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_line_protocol(df: pd.DataFrame, filename: Path):\n",
    "    df.to_csv(filename, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Line Protocol file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_line_protocol(filename: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        str(filename), \n",
    "        header=None, \n",
    "        sep='\\0', \n",
    "        names=['line']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_line_protocol(\n",
    "        write_api: influxdb_client.WriteApi, \n",
    "        df_lines: pd.DataFrame,\n",
    "        bucket: str,\n",
    "        org: str, \n",
    "        batch_size: int = 1000000) -> None:\n",
    "    for batch in range(0, len(df_lines), batch_size):\n",
    "        batch_end = min(batch + batch_size - 1, len(df_lines) - 1)\n",
    "        batch_indices = slice(batch, batch_end)\n",
    "\n",
    "        print(f\"Uploading batch of {batch_indices.stop - batch_indices.start + 1} records, from {batch_indices.start} to {batch_indices.stop}.\")\n",
    "\n",
    "        write_api.write(bucket, org, df_lines.loc[batch_indices, 'line'])\n",
    "\n",
    "    write_api.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading irem_d1 data...\n",
      "Read 53417756 records from ../data_irem/csv/irem_d1_20240716T183637.csv\n",
      "Uploading batch of 1000000 records, from 0 to 999999.\n",
      "Uploading batch of 1000000 records, from 1000000 to 1999999.\n",
      "Uploading batch of 1000000 records, from 2000000 to 2999999.\n",
      "Uploading batch of 1000000 records, from 3000000 to 3999999.\n",
      "Uploading batch of 1000000 records, from 4000000 to 4999999.\n",
      "Uploading batch of 1000000 records, from 5000000 to 5999999.\n",
      "Uploading batch of 1000000 records, from 6000000 to 6999999.\n",
      "Uploading batch of 1000000 records, from 7000000 to 7999999.\n",
      "Uploading batch of 1000000 records, from 8000000 to 8999999.\n",
      "Uploading batch of 1000000 records, from 9000000 to 9999999.\n",
      "Uploading batch of 1000000 records, from 10000000 to 10999999.\n",
      "Uploading batch of 1000000 records, from 11000000 to 11999999.\n",
      "Uploading batch of 1000000 records, from 12000000 to 12999999.\n",
      "Uploading batch of 1000000 records, from 13000000 to 13999999.\n",
      "Uploading batch of 1000000 records, from 14000000 to 14999999.\n",
      "Uploading batch of 1000000 records, from 15000000 to 15999999.\n",
      "Uploading batch of 1000000 records, from 16000000 to 16999999.\n",
      "Uploading batch of 1000000 records, from 17000000 to 17999999.\n",
      "Uploading batch of 1000000 records, from 18000000 to 18999999.\n",
      "Uploading batch of 1000000 records, from 19000000 to 19999999.\n",
      "Uploading batch of 1000000 records, from 20000000 to 20999999.\n",
      "Uploading batch of 1000000 records, from 21000000 to 21999999.\n",
      "Uploading batch of 1000000 records, from 22000000 to 22999999.\n",
      "Uploading batch of 1000000 records, from 23000000 to 23999999.\n",
      "Uploading batch of 1000000 records, from 24000000 to 24999999.\n",
      "Uploading batch of 1000000 records, from 25000000 to 25999999.\n",
      "Uploading batch of 1000000 records, from 26000000 to 26999999.\n",
      "Uploading batch of 1000000 records, from 27000000 to 27999999.\n",
      "Uploading batch of 1000000 records, from 28000000 to 28999999.\n",
      "Uploading batch of 1000000 records, from 29000000 to 29999999.\n",
      "Uploading batch of 1000000 records, from 30000000 to 30999999.\n",
      "Uploading batch of 1000000 records, from 31000000 to 31999999.\n",
      "Uploading batch of 1000000 records, from 32000000 to 32999999.\n",
      "Uploading batch of 1000000 records, from 33000000 to 33999999.\n",
      "Uploading batch of 1000000 records, from 34000000 to 34999999.\n",
      "Uploading batch of 1000000 records, from 35000000 to 35999999.\n",
      "Uploading batch of 1000000 records, from 36000000 to 36999999.\n",
      "Uploading batch of 1000000 records, from 37000000 to 37999999.\n",
      "Uploading batch of 1000000 records, from 38000000 to 38999999.\n",
      "Uploading batch of 1000000 records, from 39000000 to 39999999.\n",
      "Uploading batch of 1000000 records, from 40000000 to 40999999.\n",
      "Uploading batch of 1000000 records, from 41000000 to 41999999.\n",
      "Uploading batch of 1000000 records, from 42000000 to 42999999.\n",
      "Uploading batch of 1000000 records, from 43000000 to 43999999.\n",
      "Uploading batch of 1000000 records, from 44000000 to 44999999.\n",
      "Uploading batch of 1000000 records, from 45000000 to 45999999.\n",
      "Uploading batch of 1000000 records, from 46000000 to 46999999.\n",
      "Uploading batch of 1000000 records, from 47000000 to 47999999.\n",
      "Uploading batch of 1000000 records, from 48000000 to 48999999.\n",
      "Uploading batch of 1000000 records, from 49000000 to 49999999.\n",
      "Uploading batch of 1000000 records, from 50000000 to 50999999.\n",
      "Uploading batch of 1000000 records, from 51000000 to 51999999.\n",
      "Uploading batch of 1000000 records, from 52000000 to 52999999.\n",
      "Uploading batch of 417756 records, from 53000000 to 53417755.\n",
      "Uploading irem_d2 data...\n",
      "Read 21367100 records from ../data_irem/csv/irem_d2_20240716T183637.csv\n",
      "Uploading batch of 1000000 records, from 0 to 999999.\n",
      "Uploading batch of 1000000 records, from 1000000 to 1999999.\n",
      "Uploading batch of 1000000 records, from 2000000 to 2999999.\n",
      "Uploading batch of 1000000 records, from 3000000 to 3999999.\n",
      "Uploading batch of 1000000 records, from 4000000 to 4999999.\n",
      "Uploading batch of 1000000 records, from 5000000 to 5999999.\n",
      "Uploading batch of 1000000 records, from 6000000 to 6999999.\n",
      "Uploading batch of 1000000 records, from 7000000 to 7999999.\n",
      "Uploading batch of 1000000 records, from 8000000 to 8999999.\n",
      "Uploading batch of 1000000 records, from 9000000 to 9999999.\n",
      "Uploading batch of 1000000 records, from 10000000 to 10999999.\n",
      "Uploading batch of 1000000 records, from 11000000 to 11999999.\n",
      "Uploading batch of 1000000 records, from 12000000 to 12999999.\n",
      "Uploading batch of 1000000 records, from 13000000 to 13999999.\n",
      "Uploading batch of 1000000 records, from 14000000 to 14999999.\n",
      "Uploading batch of 1000000 records, from 15000000 to 15999999.\n",
      "Uploading batch of 1000000 records, from 16000000 to 16999999.\n",
      "Uploading batch of 1000000 records, from 17000000 to 17999999.\n",
      "Uploading batch of 1000000 records, from 18000000 to 18999999.\n",
      "Uploading batch of 1000000 records, from 19000000 to 19999999.\n",
      "Uploading batch of 1000000 records, from 20000000 to 20999999.\n",
      "Uploading batch of 367100 records, from 21000000 to 21367099.\n",
      "Uploading irem_coin data...\n",
      "Read 42734180 records from ../data_irem/csv/irem_coin_20240716T183637.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# WARNING: Be prepared for using 20GB of RAM xD\n",
    "\n",
    "for particle in [\"irem_d1\", \"irem_d2\", \"irem_coin\", \"irem_d3\"]:\n",
    "    print(f\"Uploading {particle} data...\")\n",
    "\n",
    "    # 1. get the newest CSV filename\n",
    "    csv_filename = sorted(DATA_PREROCESSED_DIR.glob(f\"{particle}_2*.csv\"))[-1]\n",
    "\n",
    "    # 2. read the CSV file\n",
    "    df = read_particles(csv_filename)\n",
    "    print(f\"Read {len(df)} records from {csv_filename}\")\n",
    "\n",
    "    # 3. convert the CSV file to line protocol\n",
    "    df_lines = convert_particles_to_line_protocol(df, particle)\n",
    "\n",
    "    # 4. (optional) save the line protocol to a file\n",
    "    line_protocol_filename = DATA_LINE_PROTOCOL_DIR / f\"{csv_filename.stem}.line\"\n",
    "    save_line_protocol(df_lines, line_protocol_filename)\n",
    "\n",
    "    # 5. upload the line protocol to InfluxDB\n",
    "    write_api = get_write_api(\n",
    "        url=URL, \n",
    "        token=TOKEN, \n",
    "        org=ORG)\n",
    "\n",
    "    upload_line_protocol(\n",
    "        write_api=write_api, \n",
    "        df_lines=df_lines, \n",
    "        bucket=BUCKET, \n",
    "        org=ORG)\n",
    "    \n",
    "    write_api.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

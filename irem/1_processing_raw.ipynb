{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1: Processing raw IREM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import gzip\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "if os.environ.get('CDF_LIB', '') == '':\n",
    "    print('No CDF_LIB environment variable found for CDF file processing.')\n",
    "from spacepy import pycdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = Path(\"srem.psi.ch/datarepo/V0/irem\")\n",
    "\n",
    "DATA_RAW = Path(\"../data_irem\") / URI\n",
    "DATA_EXTRACTED = Path(\"../data_irem/extracted/\")\n",
    "DATA_CSV = Path(\"../data_irem/csv/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching data\n",
    "\n",
    "run `0_fetch_data.py` to download the data from the SREM server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_raw_filenames():\n",
    "    filenames = []\n",
    "    for dirname in os.listdir(DATA_RAW):\n",
    "        for filename in os.listdir(DATA_RAW / dirname):\n",
    "            name = DATA_RAW / dirname / filename\n",
    "            filenames.append(name)\n",
    "    return sorted(filenames)\n",
    "\n",
    "def extract_data_raw_filename(input_filename: Path, output_filename: Path):\n",
    "    with open(input_filename, 'rb') as f:\n",
    "        data = f.read()\n",
    "        decompressed = gzip.decompress(data)\n",
    "        with open(output_filename, 'wb') as g:\n",
    "            return g.write(decompressed)\n",
    "        \n",
    "def extract_data_raw():\n",
    "    filenames = get_data_raw_filenames()\n",
    "    for filename in filenames:\n",
    "        output_filename = DATA_EXTRACTED / filename.name[:-3]\n",
    "        print(output_filename)\n",
    "        extract_data_raw_filename(filename, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data_raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDF Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CDF data\n",
    "\n",
    "Output: `pycdf.CDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_empty(filename: Path) -> bool:\n",
    "    return os.stat(filename).st_size == 0\n",
    "\n",
    "def read_cdf(cdf_path: Path) -> pycdf.CDF | None:\n",
    "    \"\"\"\n",
    "    Note: It keeps the CDF file open, so it should be closed after use.\n",
    "    \"\"\"\n",
    "    if check_file_empty(cdf_path):\n",
    "        print(f\"File {cdf_path} is empty.\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "    return pycdf.CDF(str(cdf_path))\n",
    "\n",
    "\n",
    "def read_cdf_and_cache(cdf_path: Path)-> pycdf.CDF:\n",
    "    cdf = None\n",
    "    with pycdf.CDF(str(cdf_path)) as cdf:\n",
    "        cdf = cdf.copy()\n",
    "    return cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_irem_cdfs(cdf_path: Path) -> List[pycdf.CDF]:\n",
    "    cdfs = []\n",
    "    for filename in sorted(os.listdir(cdf_path)):\n",
    "        if filename.endswith(\".cdf\"):\n",
    "            path = cdf_path / filename\n",
    "            cdf = read_cdf(path)\n",
    "            if cdf is not None:\n",
    "                cdfs.append(cdf)\n",
    "    return cdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring CDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cdf_report(cdf: pycdf.CDF):\n",
    "    print(f'Keys:')\n",
    "    print(cdf)\n",
    "\n",
    "    print(f'\\nCDF meta:')\n",
    "    print(cdf.meta)\n",
    "    for key, val in cdf.items(): \n",
    "        print(f'\\n{key} -> {val}')\n",
    "        print(val.meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV/DataFrame processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting CDF to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_to_df(cdf: pycdf.CDF) -> pd.DataFrame:\n",
    "    df = pd.DataFrame((cdf[key][...] for key in cdf.keys())).T\n",
    "    df.columns = cdf.keys()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading / Saving CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV file into a pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    return df\n",
    "\n",
    "def save_csv(df: pd.DataFrame, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate a name and save a pandas DataFrame to a CSV file\n",
    "    \"\"\"\n",
    "    latest_time = max(df['time']).strftime('%Y%m%dT%H%M%S')\n",
    "    filename = DATA_CSV.joinpath(name + \"_\" + latest_time + '.csv')\n",
    "\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing DataFrame data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_df_time(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the time column to datetime and floor it to seconds, in place.\n",
    "    \"\"\"\n",
    "    df[\"time\"] = pd.to_datetime(df['time']).dt.floor('S')\n",
    "\n",
    "    return df\n",
    "\n",
    "# def fix_df_time_start(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Filter the dataframe to only include events after September 1, 2023, in place.\n",
    "#     \"\"\"\n",
    "#     df.query(\"time >= '2023-09-01'\", inplace=True)\n",
    "\n",
    "#     return df\n",
    "\n",
    "def fix_df_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find and remove duplicates from the dataframe, in place.\n",
    "    \"\"\"\n",
    "    df.drop_duplicates(inplace=True, keep=\"first\")\n",
    "    return df\n",
    "\n",
    "def fix_sorting_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sort the dataframe by time, in place.\n",
    "    \"\"\"\n",
    "    df.sort_values(\"time\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def fix_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fix the dataframe in place.\n",
    "    \"\"\"\n",
    "    fix_df_time(df)\n",
    "    # fix_df_time_start(df)\n",
    "    fix_df_duplicates(df)\n",
    "    fix_sorting_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating DataFrame data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_df_sorted(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Verify that the dataframe is sorted by \"time\"\n",
    "    \"\"\"\n",
    "    # Find rows where the 'time' is decreasing from the previous row\n",
    "    not_sorted_mask = df['time'].diff().dt.total_seconds() < 0\n",
    "\n",
    "    # The first row can't be \"not sorted\" by definition, so we can exclude it from the mask\n",
    "    not_sorted_mask.iloc[0] = False\n",
    "\n",
    "    # Filter the DataFrame to find the not sorted rows\n",
    "    not_sorted_rows = df[not_sorted_mask]\n",
    "\n",
    "    if not df['time'].is_monotonic_increasing:\n",
    "        raise ValueError(f\"Dataframe is not sorted by time:\\n{not_sorted_rows}\")\n",
    "\n",
    "def verify_df_time_diffs(df: pd.DataFrame, \n",
    "                         max_diff_tolerance: np.timedelta64 = np.timedelta64(90, 's'), \n",
    "                         min_diff_tolerance: np.timedelta64 = np.timedelta64(500, 'ms')) -> None:\n",
    "    \"\"\"\n",
    "    Verify that the time differences between events are within tolerance.\n",
    "    If time diff >= max_diff_tolerance, just prints the warning (data holes are permitted).\n",
    "    If time diff <= min_diff_tolerance, raises an exception (possible floating point errors).\n",
    "    \n",
    "    Assumes that the dataframe is non-decreasingly sorted by \"time\".  \n",
    "    \n",
    "    There may me multiple groups of events with the same time.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): input dataframe with \"time\" column\n",
    "        max_diff_tolerance (np.timedelta64, optional): max time difference tolerance in ms (warning only)\n",
    "        min_diff_tolerance (np.timedelta64, optional): min time difference tolerance in ms (exception)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: when time differences < min_diff_tolerance (possible floating point errors)\n",
    "    \"\"\"\n",
    "\n",
    "    # get all unique \"time\" values in df\n",
    "    times = df['time'].unique()\n",
    "\n",
    "    # calc time diffs\n",
    "    time_diffs = np.diff(times)\n",
    "\n",
    "    # check if all time diffs are not larger than the tolerance\n",
    "    checks = max_diff_tolerance > time_diffs\n",
    "    if not all(checks):\n",
    "        # find all indexes of unmet conditions\n",
    "        indexes = np.where(checks == False)[0]\n",
    "\n",
    "        # create a dataframe of times\n",
    "        df_times = pd.DataFrame(times, columns=[\"time\"])\n",
    "\n",
    "        # find all holes\n",
    "        holes = [f\"{df_times.iloc[i]['time']} and {df_times.iloc[i + 1]['time']}\" for i in indexes]\n",
    "        \n",
    "        print(\"Found time holes out of tolerance at times:\", *holes, sep='\\n\\t')\n",
    "\n",
    "\n",
    "    # check if all time diffs are not smaller than the tolerance\n",
    "    # (possible floating point errors)\n",
    "    checks = min_diff_tolerance < time_diffs\n",
    "    if not all(checks):\n",
    "        # find all indexes of unmet conditions\n",
    "        indexes = np.where(checks == False)[0]\n",
    "\n",
    "        # create a dataframe of times\n",
    "        df_times = pd.DataFrame(times, columns=[\"time\"])\n",
    "\n",
    "        # find all too close values\n",
    "        too_close = [f\"{df_times.iloc[i]['time']} and {df_times.iloc[i + 1]['time']}\" for i in indexes]\n",
    "        \n",
    "        raise ValueError(\n",
    "            \"Found time values too close to each other at times \" +\n",
    "            \"(possible floating point errors):\\n\\t\" +\n",
    "            \"\\n\\t\".join(too_close))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesing data categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_irem_cdf_labels_order(cdf: pycdf.CDF) -> bool:\n",
    "    pattern = ['TC1', 'S12', 'S13', 'S14', 'S15', 'TC2', 'S25', 'C1 ', 'C2 ',\n",
    "       'C3 ', 'C4 ', 'TC3', 'S32', 'S33', 'S34']\n",
    "    return cdf[\"label_COUNTERS\"][...].tolist() == pattern\n",
    "\n",
    "def check_irem_cdfs(irem_cdfs: List[pycdf.CDF]) -> bool:\n",
    "    for cdf in irem_cdfs:\n",
    "        if not check_irem_cdf_labels_order(cdf):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def process_irem_particles(cdf: pycdf.CDF, scaler_start: int, scaler_end: int) -> pd.DataFrame:\n",
    "    # According do the IREM User Manual:\n",
    "    # CDF label_COUNTERS always are: \n",
    "    #   - [TC1 S12 S13 S14 S15 TC2 S25 C1 C2 C3 C4 TC3 S32 S33 S34]\n",
    "    #   - and are 3 characters long e.g. \"C1 \"\n",
    "    n = len(cdf[\"EPOCH\"][...])\n",
    "    times = cdf[\"EPOCH\"][...]\n",
    "\n",
    "    d_scalers = cdf[\"label_COUNTERS\"][..., scaler_start:scaler_end]\n",
    "    d = cdf[\"COUNTRATE\"][..., scaler_start:scaler_end]\n",
    "\n",
    "    time_col = np.repeat(times, len(d_scalers))\n",
    "    scaler_col = np.tile(d_scalers, n)\n",
    "    value_col = d.flatten()\n",
    "    bin_col = np.tile(np.arange(1, 1 + len(d_scalers)), n)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"time\": time_col,\n",
    "        \"scaler\": scaler_col,\n",
    "        \"value\": value_col,\n",
    "        \"bin\": bin_col\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_irem_d1(cdf: pycdf.CDF) -> pd.DataFrame:\n",
    "    # According do the IREM User Manual:\n",
    "    # label_COUNTERS[0:5] is [TC1 S12 S13 S14 S15] which is D1\n",
    "    return process_irem_particles(cdf, 0, 5)\n",
    "\n",
    "def process_irem_d2(cdf: pycdf.CDF) -> pd.DataFrame:\n",
    "    # According do the IREM User Manual:\n",
    "    # label_COUNTERS[5:7] is [TC2 S25] which is D2\n",
    "    return process_irem_particles(cdf, 5, 7)\n",
    "\n",
    "def process_irem_coincidence(cdf: pycdf.CDF) -> pd.DataFrame:\n",
    "    # According do the IREM User Manual:\n",
    "    # label_COUNTERS[7:11] is [C1 C2 C3 C4] which is D1+D2 Coincidence\n",
    "    return process_irem_particles(cdf, 7, 11)\n",
    "\n",
    "def process_irem_d3(cdf: pycdf.CDF) -> pd.DataFrame:\n",
    "    # According do the IREM User Manual:\n",
    "    # label_COUNTERS[11:15] is [TC3 S32 S33 S34] which is D3\n",
    "    return process_irem_particles(cdf, 11, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING THIS CELL CAN CONSULE ~20GB OF RAM\n",
    "\n",
    "# 1. Reading CDFs\n",
    "irem_cdfs = read_irem_cdfs(DATA_EXTRACTED)\n",
    "print(\"Found irem CDFs:\", len(irem_cdfs))\n",
    "irem_cdf_check = check_irem_cdfs(irem_cdfs)\n",
    "print(f\"Checking irem CDFs: {irem_cdf_check}\")\n",
    "if not irem_cdf_check:\n",
    "    raise ValueError(\"irem CDFs check failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(cdfs: List[pycdf.CDF], process_fn, name) -> None:\n",
    "    # WARNING: CAN CONSUME ~15GB OF RAM!!!!\n",
    "\n",
    "    # 2. Converting CDFs to DataFrames\n",
    "    print(\"Processing\")\n",
    "    df = pd.concat((\n",
    "        process_fn(cdf) for cdf in cdfs\n",
    "    ))\n",
    "    print(\"measurements:\", len(df))\n",
    "\n",
    "    # 3. Fixing DataFrames\n",
    "    fix_df(df)\n",
    "    print(\"measurements:\", len(df), \"(after fixing)\")\n",
    "\n",
    "    # 4. Verifying DataFrames\n",
    "    # ..............\n",
    "\n",
    "    # 5. Saving DataFrames to CSV\n",
    "    print(\"Saving CSVs\")\n",
    "    save_csv(df, name)\n",
    "\n",
    "    # 6. Clueaning up RAM\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: CAN CONSUME ~15GB OF RAM!!!!\n",
    "pipeline(irem_cdfs, process_irem_d1, \"irem_d1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: CAN CONSUME ~15GB OF RAM!!!!\n",
    "pipeline(irem_cdfs, process_irem_d2, \"irem_d2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: CAN CONSUME ~15GB OF RAM!!!!\n",
    "pipeline(irem_cdfs, process_irem_d3, \"irem_d3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: CAN CONSUME ~15GB OF RAM!!!!\n",
    "pipeline(irem_cdfs, process_irem_coincidence, \"irem_coin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del irem_cdfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
